{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\u003cimg src\u003d\"../images/logo.jpg\" style\u003d\"width:85px;height:85px;float:left\" /\u003e\u003ch1 style\u003d\"position:relative;float:left;display:inline\"\u003eDCGAN in Theory and Practice\u003c/h1\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\u003ca href\u003d\"https://colab.research.google.com/github/zurutech/gans-from-theory-to-production/blob/master/3.%20TFGAN/3.1.%20DCGAN%20in%20Theory%20and%20Practice.ipynb\"\u003e\n",
        "\u003cimg align\u003d\"left\" src\u003d\"https://cdn-images-1.medium.com/max/800/1*ZpNn76K98snC9vDiIJ6Ldw.jpeg\"\u003e\u003c/img\u003e\n",
        "\u003c/a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": true,
        "pycharm": {}
      },
      "source": "\u003ch1\u003eTable of Contents\u003cspan class\u003d\"tocSkip\"\u003e\u003c/span\u003e\u003c/h1\u003e\n\u003cbr\u003e\n\u003cdiv class\u003d\"toc\"\u003e\u003cul class\u003d\"toc-item\"\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#Theory\" data-toc-modified-id\u003d\"Theory-1\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e1\u0026nbsp;\u0026nbsp;\u003c/span\u003eTheory\u003c/a\u003e\u003c/span\u003e\u003cul class\u003d\"toc-item\"\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#Enter-DCGAN\" data-toc-modified-id\u003d\"Enter-DCGAN-1.1\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e1.1\u0026nbsp;\u0026nbsp;\u003c/span\u003eEnter DCGAN\u003c/a\u003e\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#Generator:-from-noise-to-insight\" data-toc-modified-id\u003d\"Generator:-from-noise-to-insight-1.2\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e1.2\u0026nbsp;\u0026nbsp;\u003c/span\u003eGenerator: from noise to insight\u003c/a\u003e\u003c/span\u003e\u003cul class\u003d\"toc-item\"\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#Deconvolution\" data-toc-modified-id\u003d\"Deconvolution-1.2.1\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e1.2.1\u0026nbsp;\u0026nbsp;\u003c/span\u003eDeconvolution\u003c/a\u003e\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#Batch-Normalization\" data-toc-modified-id\u003d\"Batch-Normalization-1.2.2\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e1.2.2\u0026nbsp;\u0026nbsp;\u003c/span\u003eBatch Normalization\u003c/a\u003e\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#generator_fn()\" data-toc-modified-id\u003d\"generator_fn()-1.2.3\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e1.2.3\u0026nbsp;\u0026nbsp;\u003c/span\u003egenerator_fn()\u003c/a\u003e\u003c/span\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#Discriminator\" data-toc-modified-id\u003d\"Discriminator-1.3\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e1.3\u0026nbsp;\u0026nbsp;\u003c/span\u003eDiscriminator\u003c/a\u003e\u003c/span\u003e\u003cul class\u003d\"toc-item\"\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#discriminator_fn()\" data-toc-modified-id\u003d\"discriminator_fn()-1.3.1\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e1.3.1\u0026nbsp;\u0026nbsp;\u003c/span\u003ediscriminator_fn()\u003c/a\u003e\u003c/span\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#Loss-function:-a-bridge-between-two-networks\" data-toc-modified-id\u003d\"Loss-function:-a-bridge-between-two-networks-1.4\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e1.4\u0026nbsp;\u0026nbsp;\u003c/span\u003eLoss function: a bridge between two networks\u003c/a\u003e\u003c/span\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#Practice---Introduction\" data-toc-modified-id\u003d\"Practice---Introduction-2\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e2\u0026nbsp;\u0026nbsp;\u003c/span\u003ePractice - Introduction\u003c/a\u003e\u003c/span\u003e\u003cul class\u003d\"toc-item\"\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#Input-Functions\" data-toc-modified-id\u003d\"Input-Functions-2.1\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e2.1\u0026nbsp;\u0026nbsp;\u003c/span\u003eInput Functions\u003c/a\u003e\u003c/span\u003e\u003cul class\u003d\"toc-item\"\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#Loading-Images\" data-toc-modified-id\u003d\"Loading-Images-2.1.1\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e2.1.1\u0026nbsp;\u0026nbsp;\u003c/span\u003eLoading Images\u003c/a\u003e\u003c/span\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#Training\" data-toc-modified-id\u003d\"Training-2.2\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e2.2\u0026nbsp;\u0026nbsp;\u003c/span\u003eTraining\u003c/a\u003e\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#TensorBoard\" data-toc-modified-id\u003d\"TensorBoard-2.3\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e2.3\u0026nbsp;\u0026nbsp;\u003c/span\u003eTensorBoard\u003c/a\u003e\u003c/span\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#Predictions\" data-toc-modified-id\u003d\"Predictions-3\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e3\u0026nbsp;\u0026nbsp;\u003c/span\u003ePredictions\u003c/a\u003e\u003c/span\u003e\u003cul class\u003d\"toc-item\"\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#predict_input_fn()\" data-toc-modified-id\u003d\"predict_input_fn()-3.1\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e3.1\u0026nbsp;\u0026nbsp;\u003c/span\u003epredict_input_fn()\u003c/a\u003e\u003c/span\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#Preparing-for-Production\" data-toc-modified-id\u003d\"Preparing-for-Production-4\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e4\u0026nbsp;\u0026nbsp;\u003c/span\u003ePreparing for Production\u003c/a\u003e\u003c/span\u003e\u003cul class\u003d\"toc-item\"\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#serving_input_receiver_fn\" data-toc-modified-id\u003d\"serving_input_receiver_fn-4.1\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e4.1\u0026nbsp;\u0026nbsp;\u003c/span\u003eserving_input_receiver_fn\u003c/a\u003e\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#Exporting-the-model-for-production\" data-toc-modified-id\u003d\"Exporting-the-model-for-production-4.2\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e4.2\u0026nbsp;\u0026nbsp;\u003c/span\u003eExporting the model for production\u003c/a\u003e\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#Local-CloudML-Predictions-testing\" data-toc-modified-id\u003d\"Local-CloudML-Predictions-testing-4.3\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e4.3\u0026nbsp;\u0026nbsp;\u003c/span\u003eLocal CloudML Predictions testing\u003c/a\u003e\u003c/span\u003e\u003cul class\u003d\"toc-item\"\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#Generate-the-test-noise\" data-toc-modified-id\u003d\"Generate-the-test-noise-4.3.1\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e4.3.1\u0026nbsp;\u0026nbsp;\u003c/span\u003eGenerate the test noise\u003c/a\u003e\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#Execute-the-test\" data-toc-modified-id\u003d\"Execute-the-test-4.3.2\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e4.3.2\u0026nbsp;\u0026nbsp;\u003c/span\u003eExecute the test\u003c/a\u003e\u003c/span\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#To-the-serving\" data-toc-modified-id\u003d\"To-the-serving-5\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e5\u0026nbsp;\u0026nbsp;\u003c/span\u003eTo the serving\u003c/a\u003e\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#NOTES\" data-toc-modified-id\u003d\"NOTES-6\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e6\u0026nbsp;\u0026nbsp;\u003c/span\u003eNOTES\u003c/a\u003e\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cspan\u003e\u003ca href\u003d\"#Links\" data-toc-modified-id\u003d\"Links-7\"\u003e\u003cspan class\u003d\"toc-item-num\"\u003e7\u0026nbsp;\u0026nbsp;\u003c/span\u003eLinks\u003c/a\u003e\u003c/span\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "! pip install tensorflow\u003d\u003d1.13.1\n",
        "#! pip install tensorflow-gpu\u003d\u003d1.13.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "import numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers import UpSampling2D, Conv2D, BatchNormalization, LeakyReLU\nfrom tensorflow.keras.layers import Reshape, Activation, Dense, Flatten, MaxPooling2D\nfrom tensorflow.keras.models import Sequential\n\n# Import tfgan from contrib\ntfgan \u003d tf.contrib.gan"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Enter DCGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "![DCGAN](images/dcgan.png)\n\nIf you take a look at [this impressive list of GANs [2\\]](#2), you would find out that DCGAN, the architecture of our choice, is a small drop in the ocean. While there may be sexier ones, very few offers the same level of clarity, performance and computational efficiency. For these reasons DCGAN is considered one of the cornerstones of this field.\n\nAlec Radford, Luke Metz and Soumith Chintala proposed the architecture in [their 2015 paper [3\\]](#3). The idea behind DCGAN is quite straightforward. Combining a set of architectural constraints with the power of CNN yielded a robust, stable and competitive model. Moreover, the architecture is simple: 4 deconvolutional layers for the `Generator` and 4 convolutional layers for the `Discriminator`. The constraints are the following:\n\n- All pooling layers are replaced with strided convolutions (discriminator) and fractionally-strided convolutions (generator).\n- Batch-normalization used in both networks.\n- Removal of the fully-connected layers (except for the discriminator output).\n- `ReLU` for all Generator layers except the output, which uses `tanh`.\n- `LeakyReLU` activation in the discriminator for all layers.\n\nAlthough there have been very recent advancements in state of the art (i.e., CoordConv, Spectral Normalization), before venturing into the bleediest of the edges, it is essential to have a firm understanding of the basic concepts. We leave you (an opinionated) list of further resources you can use to get up to speed with the most exciting researches."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "### Generator: from noise to insight\n\n![DCGAN Generator](images/dcgan_generator.png)\n\nRecalling the theoretical explanation, the Generator is the network responsible for the data-generation. It learns how to fool the discriminator so it learns how to produce realistic results. Those results are \"sampled\u0027 from the learned manifold.\n\nThe most common type of generator input is noise (i.e.: random values). However, more specialized GANs may require extra parameters. Since our full-demo uses a Deep Convolutional GAN (DCGAN), we don\u0027t need any other parameters."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "#### Deconvolution\n\nWe do not enter into the theoretical depth of the deconvolution operation. We give the intuitive idea behind this operation.\n\n\u003e When neural networks generate images, they build the images starting from low-resolution high-level descriptions. In this way, the network starts describing a \n\u003e rough representation and then fill in the details to create the final image.\n\u003e\n\u003eTo do this, we need some way to go from a lower resolution image to a higher one. We generally do this with the deconvolution operation. \n\u003eRoughly, deconvolution layers allow the model to use the points from the small image to “paint” a larger area in the bigger output image.\n\nIn practice, the deconvolution operation is often implemented by resizing, using bi-linear or nearest neighbor interpolation, followed by a convolution operation.\n"
    },
    {
      "cell_type": "markdown",
      "source": "#### Batch Normalization\n\nWe do not enter into the theoretical depth of BatchNormalization; what you need to know is that a layer of BatchNormalization normalizes the values and that TensorFlow makes it very easy to implement such operation:\n\n```python\ntf.keras.layers.BatchNormalization()\n```",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "#### generator_fn()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "def deconv2d(inputs, filters, strides\u003d(1, 1), activation\u003dtf.nn.relu):\n    \"\"\"\\\"Deconvolution\\\" layer.\n    \n    It uses upsampling with nearest neighbor interpolation to reduce the\n    presence of checkboard artifacts.\n    \"\"\"\n    \n    input_h, input_w \u003d inputs.shape[1].value, inputs.shape[2].value\n    layer_1 \u003d tf.image.resize_nearest_neighbor(\n        inputs, (2 * input_h, 2 * input_w), name\u003d\"NNUpSample2D\"\n    )\n    # Padding before convolution is used to reduce boundary artifacts\n    layer_1 \u003d tf.pad(layer_1, [[0, 0], [2, 2], [2, 2], [0, 0]], mode\u003d\"CONSTANT\")\n    layer_2 \u003d tf.layers.conv2d(\n        inputs\u003dlayer_1,\n        filters\u003dfilters,\n        kernel_size\u003d5,\n        padding\u003d\"valid\",\n        use_bias\u003dFalse,\n        activation\u003dactivation,\n        strides\u003dstrides,\n    )\n    return layer_2\n\ndef generator_fn(inputs, mode):\n    \"\"\"Generator producing images from noise.\n\n        Args:\n            noise: A single Tensor representing noise.\n            mode: tf.estimator.ModeKeys\n\n        Returns:\n            A 64x64 (None, 4096) flattened tensor whose values are\n            inside the (-1, 1) range.\"\"\"\n    is_training \u003d mode \u003d\u003d tf.estimator.ModeKeys.TRAIN\n    linear \u003d tf.layers.dense(inputs\u003dinputs, units\u003d1024 * 4 * 4, activation\u003dtf.nn.relu)\n    net \u003d tf.reshape(linear, (-1, 4, 4, 1024))\n    net \u003d tf.layers.batch_normalization(net, training\u003dis_training)\n    net \u003d deconv2d(net, 512)\n    net \u003d tf.layers.batch_normalization(net, training\u003dis_training)\n    net \u003d deconv2d(net, 256)\n    net \u003d tf.layers.batch_normalization(net, training\u003dis_training)\n    net \u003d deconv2d(net, 128)\n    net \u003d tf.layers.batch_normalization(net, training\u003dis_training)\n    net \u003d deconv2d(net, 64)\n    net \u003d tf.layers.conv2d(\n        inputs\u003dnet,\n        filters\u003d3,\n        kernel_size\u003d5,\n        padding\u003d\"same\",\n        data_format\u003d\"channels_last\",\n        use_bias\u003dFalse,\n        strides\u003d(1, 1),)\n    output \u003d tf.tanh(net)\n    return output"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Discriminator\n",
        "\n",
        "![DCGAN Discriminator](images/dcgan_discriminator.png)\n",
        "\n",
        "As for the `input_fn()` we can reuse all the code we defined for the vanilla `Estimator` describing the discriminator network architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### discriminator_fn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "def custom_conv2d(inputs, filters, strides\u003d(2,2)):\n    \"\"\"Helper layer used to instatiate `tf.layers.conv2d` with proper arguments.\"\"\"\n    layer_1 \u003d tf.layers.conv2d(\n        inputs\u003dinputs,\n        filters\u003dfilters,\n        kernel_size\u003d5,\n        padding\u003d\"same\",\n        data_format\u003d\"channels_last\",\n        use_bias\u003dFalse,\n        strides\u003dstrides,\n    )\n    layer_1 \u003d tf.nn.leaky_relu(layer_1, alpha\u003d0.2)\n\n    return layer_1\n\ndef discriminator_fn(inputs, conditioning, mode):\n    \"\"\"Build the Discriminator network.\n    Args:\n        features: a batch of images to classify, expected input shape (None, 64, 64 , 3)\n        conditioning: a batch of labels, it is used for conditioning in the some model (es Conditional GAN).\n            GANEstimator wants this parameters around, just define an arguments so that discriminator_fn is not broken.\n        mode: tf.estimator.ModeKey\n    \n    Returns:\n            The output (logits) of the discriminator.\n    \"\"\"\n    \n    # In every mode, define the model\n    is_training \u003d mode \u003d\u003d tf.estimator.ModeKeys.TRAIN\n    net \u003d custom_conv2d(inputs, filters\u003d64)\n    net \u003d custom_conv2d(net, filters\u003d128)\n    net \u003d tf.layers.batch_normalization(net, training\u003dis_training)\n    net \u003d custom_conv2d(net, filters\u003d256)\n    net \u003d tf.layers.batch_normalization(net, training\u003dis_training)\n    net \u003d custom_conv2d(net, filters\u003d512)\n    net \u003d tf.layers.batch_normalization(net, training\u003dis_training)\n    net \u003d custom_conv2d(net, filters\u003d1024)\n    net \u003d tf.layers.batch_normalization(net, training\u003dis_training)\n    net \u003d tf.reshape(net, (-1, net.shape[1] * net.shape[2] * net.shape[3]))\n    output \u003d tf.layers.dense(net, units\u003d1)\n\n    return output"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Loss function: a bridge between two networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "There are a lot of loss functions usable in GANs\u0027 architectures from very domain-specific ones to ones that are perfect for general use cases. Since our goal here falls in the latter category, we use the so-called **Non-Saturating Loss** which is the non-saturating variant of the **MinMax Loss** proposed by Goodfellow in the [original paper]().\n\nAs stated above, one of `TFGAN`\u0027s beauties is its offer of ready-to-use losses. If you cannot find the loss you want, you can create your own. \n\nWe use the following two losses:\n\n```python\ngenerator_loss_fn\u003dtfgan.losses.minimax_generator_loss\n\ndiscriminator_loss_fn\u003dtfgan.losses.minimax_discriminator_loss\n```\n\nIt is that easy."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "## Practice - Introduction\n\nHaving seen the general structure of the TFGAN library, we dive right into the model architecture.\n\nIn our showcasing of the TensorFlow API, we have built an image recognition network. GANs, however, need the training of both a Generator and a Discriminator together. This task, in the case of writing it in vanilla TensorFlow, turns out to be verbose and performance-constrained. On the other hand, in the case of Estimator API results in somewhat complicated. Thanks to GANEstimator, instead, it becomes remarkably simple."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "### Input Functions\nA `GANEstimator` object, once created, it used to train the GAN models. The `GANEstimator` `train` function takes as a parameter an \"input function\" (i.e.: `input_fn`). We have to write that `input_fn` in a specific manner. The way we need to write that function is pretty much the same as the one we would define for a standard `tf.Estimator`. The most important thing is its return value that should be one of the following:\n\n\u003e A `tf.data.Dataset` object: Outputs of Dataset object must be a tuple (features, labels) with the same constraints as below.\n\u003e \n\u003e A tuple (features, labels): Where `features` is a Tensor or a dictionary of string feature name to Tensor and `labels` is a Tensor or a dictionary of string label name to Tensor. [...]\n\nHowever, GANs training (and evaluating) needs to use two types of features: the fake ones (noise) and the real ones (real images). On the other hand, as we said, we need to be compatible with the standard `tf.Estimator` return \"signature\", (features, labels). Exploiting this return signature, we can achieve our goal. Hence, instead of returning a (features, labels) couple we return a (noise, real) couple. In practice, what we need to do is the following:\n\n1. Structure the code in the same way the `GANEstimator.train` (and `.evaluate`) function wants **function objects** as inputs.\n2. Return **noise** first and then the **real data** (the images) instead of (features, labels) couple. Optionally we can return the labels too.\n\nDuring the training, the `GANEstimator` object will internally manage the training process and automatically pass to the `generator_fn` and `discriminator_fn` the needed values."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Loading Images\n",
        "\n",
        "We use the Celeb-A dataset downloaded using tensorflow-datasets (tfds) to easily create the input function that satisfies all the requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "def get_train_input_fn(batch_size, num_epochs, noise_dim):\n",
        "    \"\"\"The input function that builds the `tf.data.Dataset` object and instantiate\n",
        "    the iterator correctly ready to be use.\n",
        "    Returns:\n",
        "        the iterator associated to the built Dataset object.\n",
        "    \"\"\"\n",
        "    \n",
        "    def convert_and_resize(features):\n",
        "        image \u003d tf.image.convert_image_dtype(features[\"image\"], dtype\u003dtf.float32)\n",
        "        image \u003d (image - 0.5) * 2\n",
        "        image \u003d tf.image.resize(image, size\u003d(64, 64))\n",
        "        features[\"image\"] \u003d image\n",
        "        return features\n",
        "\n",
        "    def _input_fn():\n",
        "        real_data \u003d tfds.load(\"celeb_a\",split\u003dtfds.Split.ALL)\n",
        "        real_data \u003d real_data.map(convert_and_resize).map(lambda feature: feature[\"image\"])\n",
        "        real_data \u003d real_data.batch(batch_size, drop_remainder\u003dTrue).repeat(num_epochs)\n",
        "        real_data_iterator \u003d real_data.make_one_shot_iterator()\n",
        "        \n",
        "        noise \u003d tf.random_normal([batch_size, noise_dim], name\u003d\"train_noise\")\n",
        "        real_batch \u003d real_data_iterator.get_next()\n",
        "        real_batch.set_shape((batch_size,) + tuple(real_batch.shape[1:]))\n",
        "        return noise, real_batch\n",
        "    return _input_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "def dcgan():\n    \n    # Hyperparameters\n    model_dir \u003d \"../logs/\"\n    batch_size \u003d 128\n    num_epochs \u003d 1\n    noise_dim \u003d 100\n    \n    # Run Configuration (it has other arguments)\n    run_config \u003d tf.estimator.RunConfig(\n        model_dir\u003dmodel_dir, save_summary_steps\u003d50, save_checkpoints_steps\u003d500)\n    \n    # Instantiate the GANEstimator object\n    gan_estimator \u003d tfgan.estimator.GANEstimator(\n        config\u003drun_config,\n        generator_fn\u003dgenerator_fn,\n        discriminator_fn\u003ddiscriminator_fn,\n        generator_loss_fn\u003dtfgan.losses.modified_generator_loss,\n        discriminator_loss_fn\u003dtfgan.losses.modified_discriminator_loss,\n        generator_optimizer\u003dtf.train.AdamOptimizer(0.0002, 0.5),\n        discriminator_optimizer\u003dtf.train.AdamOptimizer(0.0002, 0.5),\n        add_summaries\u003dtfgan.estimator.SummaryType.IMAGES\n    )\n    \n    # Instantiate the train_input_fn\n    # The model will train until it exhausts the Dataset which is repeated EPOCH times\n    train_input_fn \u003d get_train_input_fn(batch_size, num_epochs, noise_dim)\n    trained_model \u003d gan_estimator.train(train_input_fn, max_steps\u003dNone)\n    return trained_model"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### TensorBoard\n",
        "\n",
        "In order to track our training we need to launch a **TensorBoard** session pointing to the folder (`model_dir`) containing the logs generated by our training.\n",
        "\n",
        "```bash\n",
        "tensorboard --logdir\u003dPATH_TO_YOUR_MODEL_DIR\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "trained_model \u003d dcgan()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Predictions\n",
        "\n",
        "We train Machine Learning models because we want them to perform some specific task since we now have our GAN trained, we can finally use it to \"predict\" AKA generate a new image from a noise vector.\n",
        "\n",
        "Once again, TFGAN makes it as easy as invoking the `predict()`  method of our `GANEstimator` while passing to it a `predict_input_fn` as a required argument. \n",
        "\n",
        "### predict_input_fn()\n",
        "\n",
        "As previously mentioned while theoretically identical, `train_input_fn` and `predict_nput_fn()` should be implemented differently.  The first one is a simple `tf.random_normal()` node, the second should make proper use of the `tf.Dataset` API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def _predict_input_fn(batch_size, noise_dims\u003d100, **kwargs):\n",
        "    \n",
        "    def predict_input_fn():\n",
        "        noise_gen \u003d np.array([np.float32(np.random.normal(size\u003d[1, noise_dims])) for i in range(batch_size)])\n",
        "        dataset \u003d tf.data.Dataset.from_tensor_slices(noise_gen)\n",
        "        dataset \u003d dataset.batch(batch_size)\n",
        "        iterator \u003d dataset.make_one_shot_iterator()\n",
        "        return iterator.get_next()\n",
        "        \n",
        "    return predict_input_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "predictions_batch \u003d 3\n",
        "predict_input_fn \u003d _predict_input_fn(batch_size\u003dpredictions_batch)\n",
        "predictions \u003d trained_model.predict(predict_input_fn)\n",
        "[next(predictions) for _ in range(predictions_batch)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Preparing for Production\n",
        "\n",
        "Now that we have our model trained and ready we are just a couple steps away from being able to put our model into production.\n",
        "\n",
        "### serving_input_receiver_fn\n",
        "\n",
        "To serve a model in production, we first need to equip it with an interface which will receive data from our requests, such interface is specified by the aptly named `serving_input_receiver_fn`. Of the three input functions, this is the trickiest one since it has its peculiar API.\n",
        "\n",
        "This functions requires its output to be a either a `ServingInputReceiver` or a `TensorServingInputReceiver` object; the documentation on their use is clear:\n",
        "\n",
        "\u003e The normal `ServingInputReceiver` always returns a feature dict, even if it contains only one entry, and so can be used only with models that accept such a dict. \n",
        "\u003eFor models that accept only a single raw feature, the `serving_input_receiver_fn` provided to `Estimator.export_savedmodel()` should return this `TensorServingInputReceiver`.\n",
        "\n",
        "Since our model needs only a noise vector to get going, we can use `TensorServingInputReceiver`.\n",
        "\n",
        "\u003eThe expected return values are: \n",
        "\u003e - **features**: A single `Tensor` or `SparseTensor`, representing the feature to be passed to the model. \n",
        "\u003e - **receiver_tensors**: A Tensor, SparseTensor, or dict of string to Tensor or SparseTensor, specifying input nodes where this receiver expects to be fed by default. Typically, this is a single placeholder expecting serialized `tf.Example` protos. \n",
        "\u003e - **receiver_tensors_alternatives**: a dict of string to additional groups of receiver tensors, each of which may be a `Tensor`, `SparseTensor`, or dict of string to `Tensor` or `SparseTensor`. These named receiver tensor alternatives generate additional serving signatures, which may be used to feed inputs at different points within the input receiver subgraph. A typical usage is to allow feeding raw feature Tensors downstream of the `tf.parse_example()` op. Defaults to None."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def _serving_input_receiver_fn():\n",
        "    \"\"\"Instantiate a placeholder for our serving input data.\n",
        "\n",
        "    Call the custom function `serving_input_fn()`, built following\n",
        "    TensorFlow Estimator API convention, initializing the placeholder for\n",
        "    the noise we will feed the model during its serving.\n",
        "\n",
        "    The Serving Input function has two key elements:\n",
        "\n",
        "        - the data-processing step, where we concretely prepare data to be\n",
        "        fed to the:\n",
        "        - Placeholder, it is the node where the input are fed.\n",
        "\n",
        "    The things to notice is that while using `ServingInputReceiver`\n",
        "    your data processing step should have at its core the parsing of the\n",
        "    tf.Example received.\n",
        "\n",
        "    With `TensorServingInputReceiver` our data won\u0027t really be passed by the\n",
        "    request, instead it will have to be generated \u0027\u0027model-side\u0027\u0027 inside the\n",
        "    `_serving_input_receiver_fn` itself.\n",
        "\n",
        "    Returns:\n",
        "        tf.estimator.export.TensorServingInputReceiver passing the\n",
        "        placeholder for the noise to it.\n",
        "    \"\"\"\n",
        "\n",
        "    receiver_tensors \u003d tf.placeholder(shape\u003d[None, 100], dtype\u003dtf.float32, name\u003d\"serving_noise\")\n",
        "    return tf.estimator.export.TensorServingInputReceiver(receiver_tensors, receiver_tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Exporting the model for production\n",
        "\n",
        "Exporting the model is as easy as calling `GANEstimator.export_savedmodel()` which the same as the normal `Estimator`.\n",
        "\n",
        "\u003e This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator\u0027s model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single MetaGraphDef saved from this session.\n",
        "\n",
        "We have to specify an output folder and the `serving_input_receiver_fn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "export_dir_base \u003d \"../assets/exported_models\"\n",
        "trained_model.export_savedmodel(\n",
        "    export_dir_base, _serving_input_receiver_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Local CloudML Predictions testing\n",
        "\n",
        "Before going over the required steps for model serving, we want to test it locally to make sure that the exported models will behave correctly once loaded onto CloudML Engine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Generate the test noise\n",
        "\n",
        "Google Cloud often uses Newline Delimited JSON  when working with JSON-formatted data; we can use the `jsonlines` Python library assure our compliance with the standard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE \u003d 2\n",
        "NOISE_DIMS \u003d 100\n",
        "\n",
        "with jsonlines.open(\"../assets/test_noise.ndjson\", \"w\") as writer:\n",
        "    noise \u003d np.random.normal(size\u003d(BATCH_SIZE, NOISE_DIMS)).tolist()\n",
        "    noised_dict \u003d [{\"input\": n} for n in noise]\n",
        "    writer.write_all(noised_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Execute the test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "MODEL_DIR\u003d\"../assets/exported_models\"\n",
        "MODEL_ID\u003d\"1535124551\"\n",
        "JSON_INSTANCES\u003d\"../assets/test_noise.ndjson\"\n",
        "\n",
        "rm ~/google-cloud-sdk/lib/googlecloudsdk/command_lib/ml_engine/*.pyc\n",
        "\n",
        "gcloud ml-engine local predict \\\n",
        "    --model-dir\u003d$MODEL_DIR/$MODEL_ID \\\n",
        "    --json-instances\u003d$JSON_INSTANCES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## To the serving\n",
        "\n",
        "Now that we have the exported model we are ready to finally serve our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## NOTES\n",
        "\n",
        "[1]: [The GAN Landscape: Losses, Architectures, Regularization, and Normalization](https://arxiv.org/abs/1807.04720v1), which in our opinion is one of the most thorough scientific studies on GANs out there, suggests that `Spectral Normalization` is the real big deal and that `BatchNorm` actually hurt performance if applied to the Discriminator. We still decided to go with the classic formulation of DCGAN as not to overtax you with theoretical discussions. We leave the implementation of the `Spectral Normalization`(and SNGAN) to you as an exercise. On the theoretical treating of `BatchNorm` we recommend a back to back reading of [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167), [Understanding Batch Normalization](https://arxiv.org/abs/1806.02375v1), [How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)](https://arxiv.org/abs/1805.11604) \n",
        "\n",
        "[2]: While often you want to work with **features (and labels)** fetched from a `tf.Dataset`, the **noise** should always be instantiated using a simple TensorFlow node. Trying to create a noise-containing `tf.Dataset` for the `train_input_fn` is not worth the effort. NOTE: that as things are different for the [predict_input_fn](##predict_input_fn()). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "## Links\n\n\u003ca id\u003d\"1\"\u003e[1]\u003c/a\u003e: Generative Adversarial Networks : https://arxiv.org/abs/1406.2661\n\n\u003ca id\u003d\"2\"\u003e[2]\u003c/a\u003e: really-awesome-gan : https://github.com/nightrome/really-awesome-gan\n\n\u003ca id\u003d\"3\"\u003e[3]\u003c/a\u003e: DCGAN : https://arxiv.org/abs/1511.06434\n\n\u003ca id\u003d\"4\"\u003e[4]\u003c/a\u003e: Deconvolution and Checkerboard Artifacts : https://distill.pub/2016/deconv-checkerboard/\n\n\u003ca id\u003d\"5\"\u003e[5]\u003c/a\u003e: TFGAN MNIST GAN Example : https://github.com/tensorflow/models/tree/master/research/gan/mnist_estimator\n\n\u003ca id\u003d\"6\"\u003e[6]\u003c/a\u003e: Estimator API : https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator\n\n\u003ca id\u003d\"7\"\u003e[7]\u003c/a\u003e: export_savedmodel : https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_savedmodel"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": "1",
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}