{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/logo.jpg\" style=\"width:85px;height:85px;float:left\" /><h1 style=\"position:relative;float:left;display:inline\">GAN: Theory and Applications</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://colab.research.google.com/github/zurutech/gans-from-theory-to-production/blob/master/1.%20GAN%20-%20theory%20and%20applications/1.%20GAN%20-%20theory%20and%20applications.ipynb'>\n",
    "    <img align=\"left\" src='https://cdn-images-1.medium.com/max/800/1*ZpNn76K98snC9vDiIJ6Ldw.jpeg'></img>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introducing-GAN\" data-toc-modified-id=\"Introducing-GAN-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introducing GAN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generator\" data-toc-modified-id=\"Generator-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Generator</a></span></li><li><span><a href=\"#Discriminator\" data-toc-modified-id=\"Discriminator-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Discriminator</a></span></li><li><span><a href=\"#Intuitive-explaination\" data-toc-modified-id=\"Intuitive-explaination-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Intuitive explaination</a></span></li><li><span><a href=\"#Non-saturating-value-function\" data-toc-modified-id=\"Non-saturating-value-function-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Non saturating value function</a></span></li><li><span><a href=\"#Models-definition\" data-toc-modified-id=\"Models-definition-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Models definition</a></span></li><li><span><a href=\"#Training-phase\" data-toc-modified-id=\"Training-phase-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Training phase</a></span></li><li><span><a href=\"#Type-of-GANs\" data-toc-modified-id=\"Type-of-GANs-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Type of GANs</a></span></li><li><span><a href=\"#Conditional-GANs\" data-toc-modified-id=\"Conditional-GANs-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Conditional GANs</a></span></li><li><span><a href=\"#Applications\" data-toc-modified-id=\"Applications-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Applications</a></span><ul class=\"toc-item\"><li><span><a href=\"#Unconditional-GAN\" data-toc-modified-id=\"Unconditional-GAN-1.9.1\"><span class=\"toc-item-num\">1.9.1&nbsp;&nbsp;</span>Unconditional GAN</a></span></li><li><span><a href=\"#Conditional-GAN\" data-toc-modified-id=\"Conditional-GAN-1.9.2\"><span class=\"toc-item-num\">1.9.2&nbsp;&nbsp;</span>Conditional GAN</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Introducing GAN\n",
    "\n",
    "*Adversarial Training (also called GAN for Generative Adversarial Networks) is the most interesting idea in the last 10 years of ML* <sup>[1](#1)</sup>\n",
    "\n",
    "GANs are a framework for the estimation of generative models via an adversarial process in which 2 models, a **discriminator** $D$ and a **generator** $G$, are trained simultaneously.\n",
    "\n",
    "The generative model $G$ aim is to capture the data distribution, whilst the discriminative model $D$ estimates the probability that a sample came from the training data rather than $G$.\n",
    "\n",
    "![Generative Adversarial Network](slides/images/GANs.png)\n",
    "(Credit: [Silva](https://medium.freecodecamp.org/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394))\n",
    "\n",
    "The power of the adversarial training framework cames from the fact that both $D$ and $G$ can be a non-linear, parametric, mapping functions such as **neural networks** and all the network can be trained end to end using **gradient descent** or **gradient ascent** depening of we are interested in minimizing or maximizing an objective.\n",
    "\n",
    "To learn a generator distribtution $p_g$ over the data **$x$** the generator builds a mapping from a **prior** noise distribution $p_z(z)$ to a data space as $G(z;\\theta_g)$.\n",
    "\n",
    "The discriminator $D(x;\\theta_d)$ outputs a single scalar representing the probability that $x$ came from real data rather than $p_g$.\n",
    "\n",
    "The original GAN framework poses this problem as a **min-max** game in which the two players ($G$ and $D$) compete one against the other.\n",
    "\n",
    "### Generator\n",
    "\n",
    "The generator is responsible for learning a distribution as good as a sample $Y \\sim p_g$ can fool the discriminator\n",
    "\n",
    "### Discriminator\n",
    "\n",
    "The discriminator is responsibile for classifiyng the generated sample $D(Y) = D(G(z))$ in 1 of 2 different classes: **real** vs **fake**.\n",
    "\n",
    "Generator and Discriminator compete against each other, playing the following zero sum min-max game with value function $V_{GAN}(D,G)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\min_G \\max_D V_{GAN}(D,G) = \\mathbb{E} _{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuitive explaination\n",
    "\n",
    "We want to be sure to train the **discriminator** $D$ in order to correctly classify the values sampled from the real data (**maximize** $\\mathbb{E}_{x\\sim p_{data}(x)}[\\log D(x)]$ ) and **at the same time** given a fake sample $G(z), z \\sim p_z(z)$ the discriminator is expected to output a probability $D(G(z))$ close to zero, by **maximizing** $\\mathbb{E}_{z \\sim p_z(z)}[\\log(1- D(G(z)))]$.\n",
    "\n",
    "The **generator** instead, is trained in order to *fool the discriminator*, so it will learn to produce samples that are more and more similar to the ones sampled from the real data distribution and it will do this by **minimizing** $\\mathbb{E}_{z \\sim p_z(z)}[\\log(1- D(G(z)))]$\n",
    "\n",
    "Note: *the minmax game is played only in the second part of the equation, in fact when updating the generator parameters the first term has no impact*.\n",
    "\n",
    "### Non saturating value function\n",
    "\n",
    "As Goodfellow itself pointed out in the original GAN paper<sup>[2](#2)</sup> the previous equation may not provide sufficient gradient for $G$ to learn well. Early in learning, when the quality of the generated samples from $G$ is poor, $D$ can reject samples with high confidence because they are clearly different from the training data. In this case, $\\log(1 - D(G(z)))$ **saturates**.\n",
    "\n",
    "The proposed solution is to train $G$ to **maximize** $\\log D(G(z))$ instead of minimizing $\\log(1 - D(G(z)))$. This means that the 2 networks are going to optimize 2 different, but interacting, value functions (they are playing the same game, in a different manner):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "V_{GAN}(D,G) = \\begin{cases}\n",
    "D: & \\max_D  \\mathbb{E} _{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))] \\\\\n",
    "G: & \\max_G \\mathbb{E}_{z \\sim p_z(z)}[\\log(D(G(z)))]\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models definition\n",
    "\n",
    "As stated above, the power of GAN is that both $G$ and $D$ can be parametrized functions. Defining $G$ and $D$ as neural networks parametrized trough the sets of parameters $\\theta_g$ and $\\theta_d$ allow us to exploit the power of different neural networks architectures to reach different aims.\n",
    "\n",
    "In fact, there are **a lot** of different neural network architectures that can be used depending on the **data type** we are working on and on the **dataset size**.\n",
    "\n",
    "- Tuple of numbers? Fully connected (FC)\n",
    "- Text? Recurrent Neural Network (RNN)\n",
    "- Images? Convolutional Neural Networks (CNN)\n",
    "\n",
    "Every macro-category listed above, has its **huge** set of different architectures (e.g.: in the CNN domain, there are hundrends of well-known different architectures like LeNet, VGG, ResNet, SqueeezeNet, ...) and GAN authors themself suggest other architectures that worked well in their experiments.\n",
    "\n",
    "However, definining both $D$ and $G$ as parametrized funtion, allow us to train the whole GAN end to end using **gradient descent**.\n",
    "\n",
    "### Training phase\n",
    "\n",
    "Playing this min-max game consists in the **alternating** execution of training steps. As described in the \"Non saturating value function\" section, the discriminator and the generator compete against each other and they do this in turn.\n",
    "\n",
    "In order to be computationally tractable (and to make the training process more stable) it is not used the **pure gradient descent** but instead, as we usually do in most of the case when working with ML algorithms, the training process is made using **minibatch stochastic gradient descent**.\n",
    "\n",
    "The game follows this rules:\n",
    "\n",
    "**Discriminator**: this steps can be repeated from 1 to $k$ times, where $k$ is an hyperparameter.\n",
    "\n",
    "1. Sample minibatch of $m$ noise samples ${z^{(1)},\\dots,z^{(m)}}$ from noise prior $p_g(z)$\n",
    "2. Sample minibatch of $m$ examples ${x^{(1)},\\dots,x^{(m)}}$ from data generating distribution $p_{data}(x)$\n",
    "3. **Train the discriminator** by stochastic gradient **ascent**:\n",
    "$$ \\nabla_{\\theta_d} \\frac{1}{m} \\sum_{i=1}^{m}\\log D(x^{(i)}) + \\log(1 - D(G(z^{(i)}))) $$\n",
    "\n",
    "**Generator**: the update is executed only once and only after the turn of the discriminator is completed\n",
    "\n",
    "1. Sample minibatch of $m$ noise samples ${z^{(1)},\\dots,z^{(m)}}$ from noise prior $p_g(z)$\n",
    "2. **Train the generator** by stochastic gradient **ascent** (we are using the non saturating value function):\n",
    "\n",
    "$$ \\nabla_{\\theta_g} \\frac{1}{m} \\sum_{i=1}^{m} \\log(D(G(z^{(i)}))) $$\n",
    "\n",
    "The gradient-based updates can use any standard optimization algorithm, since we are using neural networks (hence we can use the momentum, Adam, RMSProp, ...)\n",
    "\n",
    "The training phase (discriminator + generator) can last for an arbitrary number of training steps (or better, for an aribitrary number of epochs, that is a better way to measure the training periods).\n",
    "\n",
    "The train can be considered completed when the discriminator is **completely fooled** by the generator: e.g. when the value of $D$ is always 0.5 that means that the best $D$ can do is only random guess, there are 2 classes and each class has equal probability (**intuitive explanation**: the samples generated are very similar to the real data and the discriminator now can only random guess).\n",
    "\n",
    "### Types of GANs\n",
    "\n",
    "Until now, we considered only one type of GAN.\n",
    "\n",
    "Actually, there are not only **thousands** of different GAN architectures<sup>[3](#3)</sup> but also GANs that can be trained to generate explicitly certain samples, adding a condition to the previously described architecture.\n",
    "\n",
    "In particular, we can classify GANs in two big families:\n",
    "\n",
    "- Unconditional GANs\n",
    "- Conditional GANs\n",
    "\n",
    "The unconditional GANs are the type of GANs just described, while the conditional GAN are a sightly modification of the GAN formulation that allows to force the generator to take into account a condition and generate only the specified type of samples.\n",
    "\n",
    "### Conditional GANs\n",
    "\n",
    "The conditional GAN paper <sup>[4](#4)</sup> clearly describes the aim and the idea of conditional GANs, so I am going to cite the paper (with some little change) in this section.\n",
    "\n",
    "GANs can be extended to a conditional model if **both** $G$ and $D$ are conditioned on some extra information $y$. $y$ could be any kind of auxiliary information, such as class labels or data from other modalities. We can perform the conditioning by feeding $y$ into both the discriminator and generator as **additional input layer**.\n",
    "\n",
    "<img src=\"slides/images/diagram.svg\" style=\"transform:rotate(90deg); width=80%\"/>\n",
    "\n",
    "In the **generator** the prior input noise $p_z(z)$ and $y$ are combined in joint hidden representation, and the adversarial training framework allows for considerable flexibility in how this hidden representation is composed.\n",
    "\n",
    "In the **discriminator** $x$ and $y$ are presented as inputs to a discriminative function (the condition has to be presented in some hidden representation).\n",
    "\n",
    "The objective function becomes:\n",
    "\n",
    "$$ \\min_G \\max_D V_{GAN}(D,G) = \\mathbb{E} _{x \\sim p_{data}(x|y)}[\\log D(x, y)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z|y), y))] $$\n",
    "\n",
    "One thing to note is **how** we present the condition $y$ as input to the networks: the same representation of the condition has to be presented to both networks. Usually, this is more easy than you tought: just concat the representation of the condition (the 1-hot encoded label? the semantic map? ...) to the input of $G$ and the input of $D$.\n",
    "\n",
    "\n",
    "### Applications\n",
    "\n",
    "Adversarial training (unconditional/conditional) has been used to solve a lot of different tasks, from generation of faces to domain translation, passing trough super resolution applications. In this section you will se some of this application, just to let you know how powerful the adversarial training framework is and how infinite are its possibilities.\n",
    "\n",
    "#### Unconditional GAN\n",
    "\n",
    "##### MNIST\n",
    "\n",
    "![Learning process of the MNIST dataset](images/mnist.gif)\n",
    "\n",
    "A gif representing the evolution over time of the generator output.\n",
    "\n",
    "##### Face Generation\n",
    "\n",
    "From Progressive Growing of GANs<sup>[5](5)</sup>:\n",
    "\n",
    "<img src=\"slides/images/incremental.jpg\" alt=\"ProGAN\" style=\"width: 80%;\"/>\n",
    "\n",
    "\n",
    "#### Conditional GAN\n",
    "\n",
    "##### MNIST\n",
    "\n",
    "From the Conditional GAN paper, the output of the generator when trained on the MNIST dataset, where each row is conditioned on one label.\n",
    "\n",
    "![Conditional MNIST](images/conditional_mnist.png)\n",
    "\n",
    "\n",
    "##### Domain Translation\n",
    "\n",
    "From the Image-to-Image Translation with Conditional Adversarial Networks paper<sup>[6](6)</sup> (*pix2pix*), here's a clever application of conditional GAN to solve some extremely hard computer vision tasks:\n",
    "\n",
    "![pix2pix](images/pix2pix.png)\n",
    "\n",
    "##### Super Resolution Application\n",
    "\n",
    "From Photo-Realistic Single Image Super Resolution Using a Generative Adversarial Network<sup>[7](7):\n",
    "\n",
    "<img src=\"slides/images/SRGAN.png\" alt=\"SRGAN\" style=\"width: 80%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "<a id=\"1\">[1]</a>: According to Yann LeCun answer on Quora: https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning/answer/Yann-LeCun\n",
    "\n",
    "<a id=\"2\">[2]</a>: Generative Adversarial Networks https://arxiv.org/pdf/1406.2661.pdf\n",
    "\n",
    "<a id=\"3\">[3]</a>: The GAN Zoo https://github.com/hindupuravinash/the-gan-zoo\n",
    "\n",
    "<a id=\"4\">[4]</a>: Conditional Generative Adversarial Nets https://arxiv.org/pdf/1411.1784.pdf\n",
    "\n",
    "<a id=\"5\">[5]</a>: Progressive Growing of GANs for Improved Quality, Stability, and Variation http://arxiv.org/abs/1710.10196\n",
    "\n",
    "<a id=\"6\">[6]</a>: Image-to-Image Translation with Conditional Adversarial Networks https://arxiv.org/pdf/1611.07004.pdf\n",
    "\n",
    "<a id=\"7\">[6]</a>: Photo-Realistic Single Image Super Resolution Using a Generative Adversarial Network http://arxiv.org/abs/1609.04802"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
