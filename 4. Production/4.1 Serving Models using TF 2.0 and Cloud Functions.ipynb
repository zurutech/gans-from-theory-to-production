{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/logo.jpg\" style=\"width:85px;height:85px;float:left\" /><h1 style=\"position:relative;float:left;display:inline\">Serving Models using TF 2.0 and Cloud Functions</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://colab.research.google.com/github/zurutech/gans-from-theory-to-production/blob/master/4.%20Production/4.1%20Serving%20Models%20using%20TF%202.0%20and%20Cloud%20Functions.ipynb'>\n",
    "    <img align=\"left\" src='https://cdn-images-1.medium.com/max/800/1*ZpNn76K98snC9vDiIJ6Ldw.jpeg'></img></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Serving-Models-using-TF-2.0-and-Cloud-Functions\" data-toc-modified-id=\"Serving-Models-using-TF-2.0-and-Cloud-Functions-4\">Serving Models using TF 2.0 and Cloud Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#AI-Platform-vs-Cloud-Functions\" data-toc-modified-id=\"AI-Platform-vs-Cloud-Functions-4.1\">AI-Platform vs Cloud Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#AI-Platform\" data-toc-modified-id=\"AI-Platform-4.1.1\">AI-Platform</a></span><ul class=\"toc-item\"><li><span><a href=\"#Supported-Tensorflow-Versions\" data-toc-modified-id=\"Supported-Tensorflow-Versions-4.1.1.1\">Supported Tensorflow Versions</a></span></li></ul></li><li><span><a href=\"#Cloud-Functions\" data-toc-modified-id=\"Cloud-Functions-4.1.2\">Cloud Functions</a></span></li><li><span><a href=\"#Additional-Notes\" data-toc-modified-id=\"Additional-Notes-4.1.3\">Additional Notes</a></span></li></ul></li><li><span><a href=\"#Deploy-Models-Using-Google-Cloud-Functions\" data-toc-modified-id=\"Deploy-Models-Using-Google-Cloud-Functions-4.2\">Deploy Models Using Google Cloud Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cold-vs-Warm-invocation\" data-toc-modified-id=\"Cold-vs-Warm-invocation-4.2.1\">Cold vs Warm invocation</a></span></li></ul></li><li><span><a href=\"#Uploading-Model-Weights\" data-toc-modified-id=\"Uploading-Model-Weights-4.3\">Uploading Model Weights</a></span><ul class=\"toc-item\"><li><span><a href=\"#Weight-format,-not-SavedModel\" data-toc-modified-id=\"Weight-format,-not-SavedModel-4.3.1\">Weight format, not SavedModel</a></span></li><li><span><a href=\"#Save-model-weights-on-Google-Cloud-Storage\" data-toc-modified-id=\"Save-model-weights-on-Google-Cloud-Storage-4.3.2\">Save model weights on Google Cloud Storage</a></span></li></ul></li><li><span><a href=\"#Writing-our-Cloud-function\" data-toc-modified-id=\"Writing-our-Cloud-function-4.4\">Writing our Cloud function</a></span><ul class=\"toc-item\"><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-4.4.1\">Requirements</a></span></li><li><span><a href=\"#main.py\" data-toc-modified-id=\"main.py-4.4.2\">main.py</a></span><ul class=\"toc-item\"><li><span><a href=\"#Imports-and-global-variables\" data-toc-modified-id=\"Imports-and-global-variables-4.4.2.1\">Imports and global variables</a></span></li><li><span><a href=\"#Preliminaries---Model-Format\" data-toc-modified-id=\"Preliminaries---Model-Format-4.4.2.2\">Preliminaries - Model Format</a></span></li><li><span><a href=\"#Model-definition\" data-toc-modified-id=\"Model-definition-4.4.2.3\">Model definition</a></span></li><li><span><a href=\"#Download-weights\" data-toc-modified-id=\"Download-weights-4.4.2.4\">Download weights</a></span></li><li><span><a href=\"#Handler---The-Cloud-Function-Entry-Point\" data-toc-modified-id=\"Handler---The-Cloud-Function-Entry-Point-4.4.2.5\">Handler - The Cloud Function Entry Point</a></span></li></ul></li></ul></li><li><span><a href=\"#Deploying-our-Cloud-function\" data-toc-modified-id=\"Deploying-our-Cloud-function-4.5\">Deploying our Cloud function</a></span><ul class=\"toc-item\"><li><span><a href=\"#Deploy-Google-Cloud-Function-code\" data-toc-modified-id=\"Deploy-Google-Cloud-Function-code-4.5.1\">Deploy Google Cloud Function code</a></span><ul class=\"toc-item\"><li><span><a href=\"#Command-Line-Deployment\" data-toc-modified-id=\"Command-Line-Deployment-4.5.1.1\">Command Line Deployment</a></span></li><li><span><a href=\"#Dashboard-Deployment\" data-toc-modified-id=\"Dashboard-Deployment-4.5.1.2\">Dashboard Deployment</a></span></li><li><span><a href=\"#Testing\" data-toc-modified-id=\"Testing-4.5.1.3\">Testing</a></span></li><li><span><a href=\"#Logging\" data-toc-modified-id=\"Logging-4.5.1.4\">Logging</a></span></li></ul></li></ul></li><li><span><a href=\"#Demo\" data-toc-modified-id=\"Demo-4.6\">Demo</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Google Cloud Logo](images/google-cloud.png)\n",
    "\n",
    "\n",
    "![Google CloudML Engine Logo](https://storage.googleapis.com/gweb-cloudblog-publish/images/CloudFunctions_TF.max-2200x2200.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer**: highly inspired from this google [blog post](https://cloud.google.com/blog/products/ai-machine-learning/how-to-serve-deep-learning-models-using-tensorflow-2-0-with-cloud-functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow 2.0** offers a number of enhancements. However, the new framework brings some challenges, like how to **deploy** TF 2.0 models. [Google Cloud Functions](https://cloud.google.com/functions/) offer a convenient way of running inference within Google Cloud infratructure allowing you to run the most recent version of this framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI-Platform vs Cloud Functions\n",
    "\n",
    "AI-Platform and Google Cloud Functions are the two best ways for deploying machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI-Platform\n",
    "\n",
    "[AI-Platform](https://cloud.google.com/ml-engine/) (previously ML-Engine):\n",
    "\n",
    "> Google Cloud Machine Learning (ML) Engine is a managed service that enables developers and data scientists to build and bring superior machine learning models to production. Cloud ML Engine offers __training__ and __prediction__ services, which can be used together or individually.\n",
    "\n",
    "**Features:**\n",
    "- __Codeless__ inference\n",
    "- __Scalable__ infrastructure\n",
    "- Pay for runs\n",
    "- __Managed__ Service\n",
    "- Separate storage of the model (and __versioning__)\n",
    "\n",
    "#### Supported Tensorflow Versions\n",
    "\n",
    "Taken from: https://cloud.google.com/ml-engine/docs/runtime-version-list (August 2019)\n",
    "![Versions](images/tf-versions-ml-engine.png)\n",
    "\n",
    "__As we can see the last TF version supported is 1.14 so there is no way by now to deploy our TF 2.0 models using AI- Platform__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Functions\n",
    "\n",
    "[Google Cloud Functions](https://cloud.google.com/functions/):\n",
    "\n",
    "> Google Cloud Functions is a lightweight compute solution for developers to create single-purpose, __stand-alone functions__ that respond to __Cloud events__ without the need to manage a server or runtime environment.\n",
    "\n",
    "**Serverless approach:**\n",
    "\n",
    "> Cloud Functions removes the work of managing servers, configuring software, updating frameworks, and patching operating systems. The software and infrastructure are __fully managed__ by Google so that you __just add code__. Furthermore, provisioning of resources happens automatically in response to events. This means that a function can __scale__ from a few invocations a day to many millions of invocations __without any work__ from you.\n",
    "\n",
    "**Features:**\n",
    "\n",
    "- Simple code for implementing inference that at the same time allows you to implement __custom logic__.\n",
    "- Great __scalability__\n",
    "- Pay for runs (you don't pay for idle servers)\n",
    "- __Custom versions__ of different frameworks (Tf 2.0 or PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Differences](images/differences-cloud-ai.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Notes\n",
    "- Google provides some __free invocations__ of GCF per month, so this kind of setup might be perfect for a toy project\n",
    "- The model, as we will see later needs to be downloaded when the cloud function is in **cold start** phase, this may take a while for big models\n",
    "- You can use Cloud Functions in combination with AI Platform Predictions (you can learn more in this [post](https://cloud.google.com/blog/products/ai-machine-learning/empower-your-ai-platform-trained-serverless-endpoints-with-machine-learning-on-google-cloud-functions))\n",
    "- Google Cloud Functions have __limited storage__ (see [here](https://cloud.google.com/functions/quotas)), so in our cases we need to upload our model to Google Cloud Storage and then download it during the first invocation of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Models Using Google Cloud Functions\n",
    "\n",
    "Our architecture and approach is very simple.\n",
    "We will __train__ the model **locally** and then upload it to **Google Cloud**. Cloud Functions will be invoked through an API request and will **download** model, **generate** an image and **answer** with the generated image.\n",
    "\n",
    "![Architecture](https://storage.googleapis.com/gweb-cloudblog-publish/images/cloud_function_Architecture_Overview.max-1000x1000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cold vs Warm invocation\n",
    "\n",
    "**Cold invocation:** The very first cold start happens when the __first request__ comes in after deployment. After that request is processed, the instance stays alive to be reused for subsequent requests. There is no predefined threshold for instance recycling. Remember: in the first invocation we also need to __download__ our model.\n",
    "\n",
    "**Warm invocation:** After the first invocation every other invocation is a **warm** one. The model is already loaded in memory and we will only perform inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Model Weights\n",
    "\n",
    "### Weight format, not SavedModel\n",
    "\n",
    "It would be great to load our model as [SavedModel](https://www.tensorflow.org/beta/tutorials/keras/save_and_restore_models) inside our Cloud Function.\n",
    "However the SavedModel format is __not__ compatible with the Cloud Functions storage system.\n",
    "\n",
    "__SavedModel__ format:\n",
    "\n",
    "```\n",
    "- model\n",
    "|\n",
    "| ----- saved_model.pb\n",
    "| ----- variables (directory)\n",
    "```\n",
    "\n",
    "However inside our Cloud Function we can __only__ write inside the `tmp` directory and we cannot create any directory inside `tmp`, so we **cannot** download a SavedModel.\n",
    "\n",
    "We need to download only the model's __weights__ after having defined our model inside the function.\n",
    "\n",
    "We can download the model weights since the __Weight format__ is:\n",
    "\n",
    "```\n",
    "- model\n",
    "|\n",
    "| ----- weights.data-**-of**\n",
    "| ----- weights.index\n",
    "```\n",
    "\n",
    "Here we do **not** have nested folders so we can simply download our weights inside the `tmp` directory.\n",
    "\n",
    "__Remember__: to export weights from a keras model `G` you just need to run:\n",
    "\n",
    "```python\n",
    "G.save_weights('generator')\n",
    "```\n",
    "\n",
    "### Save model weights on Google Cloud Storage\n",
    "\n",
    "After the `G.save_weights('generator')` command of the previous notebook you should have the weights of the generator network inside the directory `3. AshPy`. We need to copy the network weights to Google Cloud Storage to have them available from the Cloud Function.\n",
    "\n",
    "Upload the weights using the `gsutil` command line tool:\n",
    "\n",
    "```bash\n",
    "cd 3.\\ AshPy\n",
    "mkdir dcgan\n",
    "cp generator* dcgan\n",
    "gsutil cp -r dcgan gs://euroscipy-2019-workshop/dcgan-weights\n",
    "```\n",
    "\n",
    "After these commands we will find the model weights inside the bucket `euroscipy-2019-workshop/dcgan-weights`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing our Cloud function\n",
    "\n",
    "### Requirements\n",
    "\n",
    "One of the main __upsides__ of Cloud Functions is that you don't have to manually generate the package. You can just use a __requirements.txt__ file and list all used libraries there.\n",
    "\n",
    "Let's see our __requirements.in__ file:\n",
    "\n",
    "```\n",
    "numpy\n",
    "tensorflow==2.0.0rc0\n",
    "Pillow\n",
    "google-cloud-storage\n",
    "\n",
    "# Dev\n",
    "Flask\n",
    "black\n",
    "pylint\n",
    "flake8\n",
    "flake8-bugbear\n",
    "isort\n",
    "mypy\n",
    "pytest\n",
    "pytest-cov\n",
    "pydocstyle\n",
    "opencv-python\n",
    "```\n",
    "\n",
    "To obtain the correct __requirements.txt__ file we use the `pip-compile` tool, from `pip-tools`. We just need to run the command:\n",
    "\n",
    "```\n",
    "pip-compile requirements.in\n",
    "```\n",
    "\n",
    "and we obtain our __requirements.txt__ file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main.py\n",
    "\n",
    "#### Imports and global variables\n",
    "Remember to have the model as a **global variable** in your python code so it will be **cached** and **reused** in **warm** invocations of Cloud Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Serving a TF 2.0 Model via Cloud Function.\n",
    "\n",
    "More info: http://bit.ly/310wRZl\n",
    "\"\"\"\n",
    "import base64  # needed for image encoding\n",
    "import os\n",
    "import tempfile\n",
    "from io import BytesIO\n",
    "from typing import TYPE_CHECKING, List\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "\n",
    "from flask import Request\n",
    "\n",
    "# Download model configuration\n",
    "DOWNLOAD_CONFIG = {\n",
    "    \"bucket_name\": \"euroscipy-2019-workshop\",  # name of the bucket\n",
    "    \"model_id\": \"dcgan-weights\",  # name of the model\n",
    "    \"destination_folder\": tempfile.gettempdir(),  # tmp directory\n",
    "}\n",
    "\n",
    "# Header definitions needed for the response (Needed for CORS)\n",
    "# CORS: https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS)\n",
    "headers = {\n",
    "    \"Access-Control-Allow-Headers\": \"Content-Type\",\n",
    "    \"Access-Control-Allow-Origin\": \"*\",\n",
    "}\n",
    "\n",
    "# Model variables\n",
    "\n",
    "# Dimension of the latent space of the model\n",
    "LATENT_DIMENSION = 100\n",
    "\n",
    "# We keep model as global variable so we don't have to reload it in case of warm invocations\n",
    "MODEL: tf.keras.Model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition\n",
    "\n",
    "Since we are using the __weight format__ we need to define our model inside the Cloud Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: same definition as the previous notebook\n",
    "def get_model():\n",
    "    G = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Dense(\n",
    "                1024 * 4 * 4, use_bias=False, input_shape=(LATENT_DIMENSION,)\n",
    "            ),\n",
    "            tf.keras.layers.Reshape((4, 4, 1024)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                256, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                128, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                64, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                3, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False\n",
    "            ),\n",
    "            tf.keras.layers.Activation(tf.math.tanh),\n",
    "        ]\n",
    "    )\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download weights\n",
    "\n",
    "Weights can be downloaded using this utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blobs(model_id: str, destination_folder: str, bucket_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Download all the models related blobs from the bucket.\n",
    "\n",
    "    Args:\n",
    "        model_id (str): ID of the model we want to download AKA the name of the folder on\n",
    "            GCS. This is used as a prefix filtering out all unneeded blobs.\n",
    "        destination_folder (str): Path of the destination folder where blobs will be downloaded.\n",
    "        bucket_name (str): Name of the bucket to use as target storage.\n",
    "\n",
    "    Returns:\n",
    "        :py:obj:`list` of [:py:obj:`str`]: Returns a list containing the path of the stored SavedModel(s).\n",
    "    \"\"\"\n",
    "    weights_uri: str = \"\"  # uri of the index file for weights\n",
    "\n",
    "    prefix = model_id\n",
    "\n",
    "    # Instantiate a google storage client\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # get the bucket we are interested in\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # List blobs iterate in folder\n",
    "    blobs = bucket.list_blobs(prefix=prefix)  # Excluding folder inside bucket\n",
    "    for blob in blobs:\n",
    "        print(\"Downloading \", blob.name)\n",
    "        blob_name = blob.name.split(os.path.sep)[-1]\n",
    "        if blob_name != \"\":\n",
    "            destination_uri = os.path.join(destination_folder, blob_name)\n",
    "            # We are downloading the blob of a file\n",
    "            try:\n",
    "                blob.download_to_filename(destination_uri)\n",
    "                if blob.name.endswith(\".index\"):\n",
    "                    weights_uri = destination_uri\n",
    "            except IsADirectoryError:\n",
    "                # We cannot download the blob of a folder\n",
    "                continue\n",
    "\n",
    "    if weights_uri == \"\":\n",
    "        raise ValueError(f\"No index file found in {bucket_name}/{model_id}\")\n",
    "\n",
    "    # return the uri of the weights, needed for loading weights\n",
    "    return weights_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handler - The Cloud Function Entry Point\n",
    "\n",
    "The __handler__ is the function triggered when a request arrives. This is the __entry point__ of our Google Cloud Function.  \n",
    "Here we can implement our custom logic, adding pre or post-processing.\n",
    "\n",
    "We need to perform the following steps in our handler:\n",
    "\n",
    "- __Download model__ (if needed)\n",
    "- __Load weights__ (if needed)\n",
    "- __Inference__ (`model.call()`)\n",
    "- __Post-Processing__\n",
    "- __Compose Response__ for the client\n",
    "    \n",
    "In our example we use the generator defined before to generate an image.  \n",
    "We support the generation from a latent vector (we will see later why) and a generation using a new latent vector.\n",
    "So, in our function we need to parse the request, searching for a latent vector, if the latent vector is not present we generate a new latent vector (`tf.random.normal((1, LATENT_DIMENSION))`).  \n",
    "We call the model using the latent vector as input. Then we scale correctly the output of the model and return the encoded representation of the image.\n",
    "\n",
    "We need to define our post-processing function that encodes the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(output: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    Post process the model's output\n",
    "    Args:\n",
    "        output: output image to process\n",
    "\n",
    "    Returns:\n",
    "        The image encoded as str (base64)\n",
    "    \"\"\"\n",
    "    pil_img = Image.fromarray(output)\n",
    "    buff = BytesIO()\n",
    "    image_format = \"PNG\"\n",
    "    pil_img.save(buff, format=image_format)\n",
    "    encoded_img = base64.b64encode(buff.getvalue()).decode(\"utf-8\")\n",
    "    return encoded_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally our entry point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(request: Request = None):\n",
    "    \"\"\"\n",
    "    Entry point of the Serveless call.\n",
    "\n",
    "    Args:\n",
    "        request (:py:class:`flask.Request`): Flask Request holding our payload.\n",
    "\n",
    "    \"\"\"\n",
    "    global MODEL\n",
    "    # Model load which only happens during cold starts\n",
    "    if not MODEL:\n",
    "        # The model is not defined, we need to instantiate it and download its weights\n",
    "        print(\"Cold start: Loading model\")\n",
    "        print(\"Downloading weights\")\n",
    "        weights_path = download_blobs(**DOWNLOAD_CONFIG)\n",
    "\n",
    "        # instantiate the model\n",
    "        MODEL = get_model()\n",
    "\n",
    "        # load weights\n",
    "        MODEL.load_weights(weights_path.replace(\".index\", \"\"))\n",
    "\n",
    "        print(\"weights loaded\")\n",
    "    else:\n",
    "        # the model is already defined, we are in warm start phase\n",
    "        print(\"Warm start: Using cached model\")\n",
    "\n",
    "    # log the request\n",
    "    print(\"Received\", request)\n",
    "\n",
    "    # get the request payload\n",
    "    request = request.get_json(silent=True)\n",
    "\n",
    "    # get the noise vector if present\n",
    "    try:\n",
    "\n",
    "        # use the fed noise\n",
    "        noise = tf.constant(np.array(request[\"noise_vector\"]))\n",
    "\n",
    "        # check the correct dimensions\n",
    "        if noise.shape != (1, LATENT_DIMENSION):\n",
    "            return ({}, 422, headers)\n",
    "\n",
    "        print(\"Using fed noise\")\n",
    "    except Exception:\n",
    "        # the noise vector is not present, we need to generate a new vector\n",
    "        print(\"No noise vector provided, generating noise\")\n",
    "        noise = tf.random.normal((1, LATENT_DIMENSION))\n",
    "\n",
    "    # call the model\n",
    "    output = MODEL.call(noise).numpy().squeeze()\n",
    "\n",
    "    # back in the rage [0, 255]\n",
    "    output = ((output + 1) * 127.5).astype(np.uint8)\n",
    "\n",
    "    # postprocess stage (encode)\n",
    "    encoded_image = postprocess(output)\n",
    "\n",
    "    # compose the output response\n",
    "    return (\n",
    "        {\n",
    "            \"base64_image\": encoded_image,\n",
    "            \"format\": \"png\",\n",
    "            \"noise_vector\": noise.numpy().tolist(),\n",
    "        },\n",
    "        200,\n",
    "        headers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying our Cloud function\n",
    "\n",
    "\n",
    "### Deploy Google Cloud Function code\n",
    "\n",
    "#### Command Line Deployment\n",
    "\n",
    "Reference: [Gcloud Function Deploy](https://cloud.google.com/sdk/gcloud/reference/functions/deploy).\n",
    "\n",
    "We can easily deploy and run Cloud Functions using `gcloud`:\n",
    "\n",
    "```bash\n",
    "cd 4.\\ Production/deploy\n",
    "gcloud functions deploy euroscipy-2019-tutorial --entry-point handler --stage-bucket zuru-ml-cloud-functions --runtime python37 --trigger-http --memory 2048\n",
    "```\n",
    "\n",
    "Parameters:\n",
    "- `euroscipy-2019-tutorial`: ID of the function or fully qualified identifier for the function.\n",
    "- `--entry-point handler`: Name of a Google Cloud Function (as defined in source code) that will be executed. \n",
    "- `--stage-bucket zuru-ml-cloud-functions`: When deploying a function from a local directory, this flag's value is the name of the Google Cloud Storage bucket in which source code will be stored.\n",
    "- `--runtime python37`: Runtime in which to run the function.\n",
    "- `--trigger-http`: Function will be assigned an endpoint, which you can view by using the describe command.\n",
    "- `--memory 2048`: Limit on the amount of memory the function can use.\n",
    "\n",
    "#### Dashboard Deployment\n",
    "\n",
    "You can also upload the function from the [dashboard](https://console.cloud.google.com/functions/list?_ga=2.247640449.-709977754.1558685959)\n",
    "\n",
    "![Dashboard](https://storage.googleapis.com/gweb-cloudblog-publish/images/cloud_functions_create_function.max-1200x1200.png)\n",
    "\n",
    "Next, set the main.py and requirements.txt files.\n",
    "\n",
    "Finally, push the “Create” button and initialize the creation of the function.\n",
    "\n",
    "\n",
    "#### Testing\n",
    "\n",
    "Once the function is deployed, you can test it in the “Testing” section of the Cloud Functions dashboard.\n",
    "You can also customize incoming events and see output as well as logs.\n",
    "\n",
    "![Testing](images/testing.png)\n",
    "\n",
    "\n",
    "#### Logging\n",
    "\n",
    "You can also read logs from the command line:\n",
    "\n",
    "```bash\n",
    "gcloud functions logs read euroscipy-2019-tutorial\n",
    "\n",
    "```\n",
    "\n",
    "## Demo\n",
    "\n",
    "- Generation Demo: http://zurutech.github.io/gans-from-theory-to-production.  \n",
    "- Interpolation demo: http://zurutech.github.io/gans-from-theory-to-production/interpolation.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": "4",
   "nav_menu": {
    "height": "380.99px",
    "width": "313.75px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
