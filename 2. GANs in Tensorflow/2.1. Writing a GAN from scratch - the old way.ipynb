{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing an GAN: The Old Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://colab.research.google.com/github/zurutech/gans-from-theory-to-production/blob/master/2.%20GANs%20in%20Tensorflow/2.1.%20Writing%20a%20GAN%20from%20scratch%20-%20the%20old%20way.ipynb'>\n",
    "    <img align=\"left\" src='https://cdn-images-1.medium.com/max/800/1*ZpNn76K98snC9vDiIJ6Ldw.jpeg'></img>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Writing-an-GAN-from-scratch:-The-Old-Way\" data-toc-modified-id=\"Writing-an-GAN-from-scratch:-The-Old-Way-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Writing an GAN from scratch: The Old Way</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tensorflow:-nodes'-scope\" data-toc-modified-id=\"Tensorflow:-nodes'-scope-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Tensorflow: nodes' scope</a></span></li><li><span><a href=\"#What-does-a-GAN-learn?\" data-toc-modified-id=\"What-does-a-GAN-learn?-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>What does a GAN learn?</a></span></li><li><span><a href=\"#Input-data\" data-toc-modified-id=\"Input-data-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Input data</a></span></li><li><span><a href=\"#Generator-and-discriminator-networks\" data-toc-modified-id=\"Generator-and-discriminator-networks-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Generator and discriminator networks</a></span></li><li><span><a href=\"#Define-input-and-instantiate-networks\" data-toc-modified-id=\"Define-input-and-instantiate-networks-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Define input and instantiate networks</a></span></li><li><span><a href=\"#The-loss-function-and-the-training-procedure\" data-toc-modified-id=\"The-loss-function-and-the-training-procedure-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>The loss function and the training procedure</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discriminator-loss-function\" data-toc-modified-id=\"Discriminator-loss-function-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Discriminator loss function</a></span></li><li><span><a href=\"#Generator-loss-function\" data-toc-modified-id=\"Generator-loss-function-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;</span>Generator loss function</a></span></li></ul></li><li><span><a href=\"#Gradient-ascent\" data-toc-modified-id=\"Gradient-ascent-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Gradient ascent</a></span><ul class=\"toc-item\"><li><span><a href=\"#Visualize-training\" data-toc-modified-id=\"Visualize-training-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>Visualize training</a></span></li><li><span><a href=\"#Advantages-and-disadvantages\" data-toc-modified-id=\"Advantages-and-disadvantages-1.7.2\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;</span>Advantages and disadvantages</a></span></li></ul></li></ul></li><li><span><a href=\"#Bonus-exercise:-converting-it-to-a-Conditional-GAN\" data-toc-modified-id=\"Bonus-exercise:-converting-it-to-a-Conditional-GAN-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Bonus exercise: converting it to a Conditional GAN</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing an GAN from scratch: The Old Way\n",
    "\n",
    "Tensorflow allows us to define computational graphs. We can hence describe both the generator $G$ and the discriminator $D$, just like two interacting subgraphs.\n",
    "\n",
    "A tensorflow specific feature that we have to take into account when we want to define a GAN in the \"old\" way (aka without using a `tf.estimator`) is the node's scope.\n",
    "\n",
    "### Tensorflow: nodes' scope\n",
    "\n",
    "Every node (variable and op) in the computational graph has a unique name. The naming system of tensorflow works like a filesystem directory structure:\n",
    "\n",
    "```\n",
    "/root/tree/leaf_1\n",
    "/root/tree/leaf_2\n",
    "```\n",
    "\n",
    "In this case, the scope of both `leaf_1` and `leaf_2` is `/root/tree`. Obviously, under the same scope is **not** possibile to have 2 nodes with the same name.\n",
    "\n",
    "When describing the discriminator, we'll need to feed to the network both the real and the generated samples. We could do this in two different ways:\n",
    "\n",
    "1. Manually creating the input batches with the real and the generated sample concatenated along the first dimension and do the same for the labels.\n",
    "2. Exploit the `tf.variable_scope` `reuse` feature, that allow us to define two different graphs that shares the same variables and use them to separetely feed the real and the generated data.\n",
    "\n",
    "We're going to use this second option because it's easier to use and understand (and more elegant).\n",
    "\n",
    "### What does a GAN learn?\n",
    "\n",
    "The aim of the GAN we're going to implement is to learn a certain **data distribution** present in the training set.\n",
    "\n",
    "The following exaple will guide you trough the required steps to build a GAN from scratch using Tensorflow while giving you a **visual intuition** of what a GAN learns during it's training process.\n",
    "\n",
    "### Input data\n",
    "\n",
    "In order to give a meaningful example, we're going to generate and visualize the distribution that has to be learn.\n",
    "\n",
    "$$ \\mathcal{N}(\\mu = 10, \\sigma = 0.1) $$\n",
    "Our **target** distribution is a Gaussian distribution centered in $10$ a with a small variance of $0.1$\n",
    "\n",
    "We're going to use Tensorflow itself to **sample datapoints** from this distribution in order to build our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required inclues to create the GAN and to visualize the data\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow needs to first build a graph and then place it into a `Session` object. This object represents the computation executed on a *physical device* (GPU/CPU/TPU). So, in order to use tensorflow to generate the input data, I'm going to create an `InteractiveSession` (that's just a normal session, that installs itself as the default session of this environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample `dataset_size` datapoints from the target distribtuion: this will be our training set. Once sampled, we just plot the histogram of the datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_size = 2000\n",
    "z = tf.random_normal(mean=10., shape=(), stddev=0.1, dtype=tf.float32)\n",
    "\n",
    "z_samples = [sess.run(z) for _ in range(dataset_size)]\n",
    "\n",
    "counts, bin, ignored = plt.hist(z_samples, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator and discriminator networks\n",
    "\n",
    "The **generator** $G$ network architecture is **completely arbitrary**: practice suggests that a simple layer with just 2 fully connected layers and a single linear layer at the output is enough for learning such a simple task.\n",
    "\n",
    "The **discriminator** $D$ network architecture is **completely arbitrary** too. The only thing that's mandatory is the use of the **linear activation** in the single output neuron.\n",
    "\n",
    "This is due to the fact that we're going to use a tensorflow built-in loss function to train this classifier and this function requires an \"unscaled logits\" as input vector, aka a linear output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(inputs):\n",
    "    \"\"\"generator network.\n",
    "    Args:\n",
    "        inputs: a (None, latent_space_size) tf.float32 tensor\n",
    "    Returns:\n",
    "        G: the generator output node\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        fc1 = tf.layers.dense(inputs, units=64, activation=tf.nn.elu, name=\"fc1\")\n",
    "        fc2 = tf.layers.dense(fc1, units=64, activation=tf.nn.elu, name=\"fc2\")\n",
    "        G = tf.layers.dense(fc1, units=1, name=\"G\")\n",
    "    return G\n",
    "    \n",
    "def disciminator(inputs, reuse=False):\n",
    "    \"\"\"discriminator network.\n",
    "    Args:\n",
    "        inputs: a (None, 1) tf.float32 tensor\n",
    "        reuse: python boolean, if we expect to reuse (True) or declare (False) the variables\n",
    "    Returns:\n",
    "        D: the discriminator output node\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        fc1 = tf.layers.dense(inputs, units=32, activation=tf.nn.elu, name=\"fc1\")\n",
    "        D = tf.layers.dense(fc1, units=1, name=\"D\")\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: we definied both the discriminator and the generator **under different scopes**, for the disciminator we also passed the **reuse** parameter to `tf.variable_scope`. This will help us to pass to the discriminator the generated and the real data in a elegant manner.\n",
    "\n",
    "### Define input and instantiate networks\n",
    "\n",
    "So far we just defined the $G$ and $D$ architecture, without creating any instance of them.\n",
    "\n",
    "What's missing is to define the shape of the inputs (we'll use `tf.placeholder`) and define the computational graph.\n",
    "\n",
    "What we're going to do in the next few lines is to **exploit** the `reuse` parameter of `tf.variable_scope` to **define the discriminator parameters once**, but define two different computational graphs that **share** the same parameters.\n",
    "\n",
    "1. A graph for the real_input $D(x), x \\sim p_{data}$\n",
    "1. A graph for the generated data $D(G(z))$\n",
    "\n",
    "For the generator, instead, the definition is straightforward: just accept noise as input and produce the generated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the real input, a batch of values sampled from the real data (our N(10,0.1) distribution)\n",
    "real_input = tf.placeholder(tf.float32, shape=(None,1))\n",
    "# Define the discriminator network and its parameters\n",
    "D_real = disciminator(real_input)\n",
    "\n",
    "# Arbitrary set the shape of the noise prior (vector of 100 values sampled from a N(0,1) distribution)\n",
    "latent_space_size = 100\n",
    "# Define the input noise shape and define the generator\n",
    "input_noise = tf.placeholder(tf.float32, shape=(None,latent_space_size))\n",
    "G = generator(input_noise)\n",
    "\n",
    "# now that we have defined the generator and G is the generator output, we can define another discriminator graph\n",
    "# that will **reuse** the varibles defined previously\n",
    "D_fake = disciminator(G, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss function and the training procedure\n",
    "\n",
    "As the theory describes, the training procedure is the **alternating** execution of training steps. In the next few lines we're going to implement the **non saturating** version of the value function.\n",
    "\n",
    "As described above, the output layers of the 2 models have **linear output** because we're going to use the `tf.nn.sigmoid_cross_entropy_with_logits` that wants unscaled logits as input.\n",
    "\n",
    "In practice, the `tf.nn.sigmoid_cross_entropy_with_logits` computes the binary cross entropy between two distributions:\n",
    "\n",
    "1. the learned distribution, that assigns a probability to a certain class, that's why the function applies the **sigmoid**  $\\sigma$ function to the output neuron, in order to consider the output a probability (range [0-1])\n",
    "2. the conditional empirical distribution over class labels (a probability distribution where the probability of the current observed positive sample is 1 and is 0 for any other class)\n",
    "\n",
    "$$ \\mathcal{L}_{BCE} = y - \\log(\\hat{y}) - (1 - y)\\log(1 - \\hat{y}) $$\n",
    "\n",
    "#### Discriminator loss function\n",
    "\n",
    "Given the labels for the positive class and the nevative class to be 1 and 0 respectively, the loss becomes the sum of 2 BCE:\n",
    "\n",
    "$$ \\frac{1}{m} \\sum_{i=1}^{m}- \\log \\sigma(D(x^{(i)})) + \\frac{1}{m} \\sum_{i=1}^{m} - \\log(1 - \\sigma(D(G(z^{(i)})) $$\n",
    "\n",
    "The loss function is the binary cross entropy between the class of the real samples (label 1) and the class of the generated samples (label 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_loss_real = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real, labels=tf.ones_like(D_real))\n",
    ")\n",
    "\n",
    "D_loss_fake = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake, labels=tf.zeros_like(D_fake))\n",
    ")\n",
    "\n",
    "D_loss = D_loss_real + D_loss_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator loss function\n",
    "$$ -\\frac{1}{m} \\sum_{i=1}^{m} \\sigma(\\log(D(G(z)))) $$\n",
    "\n",
    "The loss function is just the binary cross entropy between the log probability of the generated images and the distribution of the real images (label 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_loss = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake, labels=tf.ones_like(D_fake))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient ascent\n",
    "\n",
    "In order to train 2 different networks one next to the other, we have to do something a little bit unusual when using tensorflow.\n",
    "\n",
    "In the majority of the examples online, you'll find the usage of a single `tf.train.*Optimizer` used to train all the variables (gathered using `tf.trainable_variables()` or just falling back on the default behaviour).\n",
    "\n",
    "When training a GAN, instead, we want to **specify the variables we want to train on each step**, and hence define also more than one optimzer.\n",
    "\n",
    "Having correctly specified the scope of the generator and of the descriminator, we can be sure that the generator and the disciminator variable are in the `/generator` and `/discriminator` scope respectively.\n",
    "\n",
    "Hence, we can use `tf.trainable_variables` `scope` parameter to gather all the variables declared under the specified scope.\n",
    "\n",
    "*tip*: if the optimizer is the same in both $G$ and $D$ you can use the same optimizer, just by calling the `.minimize` method on different sets of variables. The choose of the optimizer is an additional hyperparameter of the GAN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather D and G variables\n",
    "D_vars = tf.trainable_variables(scope=\"discriminator\")\n",
    "G_vars = tf.trainable_variables(scope=\"generator\")\n",
    "\n",
    "# Define the optimizers and the train operations\n",
    "train_D = tf.train.AdamOptimizer(1e-5).minimize(D_loss, var_list=D_vars)\n",
    "train_G = tf.train.AdamOptimizer(1e-5).minimize(G_loss, var_list=G_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to play the game.\n",
    "\n",
    "The training process is **exactly** the one described in the GAN paper.\n",
    "\n",
    "Just a note about the **discriminator train**: we're passing the noise vector even in this step, this is required because when calling `train_D` we're calling `D_loss` that requires the `fake_data` generated by $G(z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4270c412cd76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Passing from a vector of `(dataset_size,)` shape to a batch with shape `(dataset_size, 1)`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mreal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# mu=10, mean=0.1, training_set size samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Add a dimension to the training set, in order to get the correct shape for training (being compatible with the\n",
    "# shape of the placeholder input_noise ).\n",
    "# Passing from a vector of `(dataset_size,)` shape to a batch with shape `(dataset_size, 1)`\n",
    "\n",
    "real_data = np.expand_dims(np.array(z_samples),1) # mu=10, mean=0.1, training_set size samples\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Let's play the min-max game\n",
    "\n",
    "if not os.path.exists(\"./gif/\"):\n",
    "     os.makedirs(\"./gif/\")\n",
    "\n",
    "for step in range(100000):\n",
    "    noise_vector = np.random.normal(loc=0, scale=1, size=(dataset_size, latent_space_size))\n",
    "    \n",
    "    # Train the disciminator\n",
    "    _, d_loss_value = sess.run([train_D, D_loss], feed_dict={real_input: real_data, input_noise: noise_vector})\n",
    "    \n",
    "    # Train the generator\n",
    "    _, g_loss_value = sess.run([train_G, G_loss], feed_dict={input_noise: noise_vector})\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(\"G loss: \", g_loss_value, \" D loss: \", d_loss_value, \" step: \", step)\n",
    "        \n",
    "        # Sample 5000 values from the Generator and draw the histogram\n",
    "        sampled = sess.run(G, feed_dict={input_noise: noise_vector})\n",
    "        counts, bin, ignored = plt.hist(sampled, 100, range=(-4, 14))\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(pl.gcf())\n",
    "        plt.savefig(\"./gif/{}.png\".format(step))\n",
    "        plt.gca().clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize training\n",
    "\n",
    "Every 100 steps of train, we generated and saved an histogram of (`dataset_size`) data points.\n",
    "In order to generate a nice animation of the learning process we can just use the `imagemagick` tool to merge all the images into a pretty cool gif of the learning process.\n",
    "This gif shows you how the learning process learned to shift the initial random distribution to the correct value of $10$ and to adjust the variance in order to be close to the one of **target distribution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "! convert -delay 20 -loop 0 $(ls gif/*.png | sort -V) gif/learning_gaussian.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![animation](gif/learning_gaussian.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages and disadvantages\n",
    "\n",
    "Writing a GAN from scratch using only the plain old Tensorflow API is extremely helpful for learning how to correctly write a GAN, understand the theory behinds this tool and how to deal with the peculiarities of Tensorflow (scoping, graph definition, variable reuse, how to define the loss functions, ...)\n",
    "\n",
    "However, this example could become extremely long and boring if we add:\n",
    "\n",
    "- The dataset generation (using the Tensorflow QueueRunner - manually handle multi threading)\n",
    "- The logging: use `tf.summary.*` methods to log metrics + tf.Saver to log summaries\n",
    "- The usage of `tf.Saver` to log the summaries and to save the trained model itself\n",
    "- The data visualization in tensorbard\n",
    "- The export of the model with the correct input definition in order to be ready for serving\n",
    "- ...\n",
    "\n",
    "In order to speed-up the development time, the tensorflow devs introduced the `tf.estimator`<sup>[1](#1)</sup> and `tf.data`<sup>[2](#1)</sup> API: an high level API that greatly simplifies machine learning programming. Estimators encapsulate the following actions:\n",
    "\n",
    "- training\n",
    "- evaluation\n",
    "- prediction\n",
    "- export for serving\n",
    "\n",
    "While the `tf.data` API made the dataset generation extremely easy.\n",
    "\n",
    "In the next chapter, we'll see how to use the `tf.estimator` and `tf.data` to create a dataset and to train a GAN.\n",
    "\n",
    "We'll start with the definition of the data and of the discriminator.\n",
    "\n",
    "## Bonus exercise: converting it to a Conditional GAN\n",
    "\n",
    "Extending this simple GAN making it conditional can be a good test bench to check your understanding of the GAN theory explained in the first section.\n",
    "\n",
    "Remeber that both $G$ and $D$ needs to be conditioned and that the train process is just the same game, played for every different condition.\n",
    "\n",
    "Copy this notebook and try to condition this GAN in order to learn 2 different normal distributions.\n",
    "\n",
    "- Condition `0`: generates $\\mathcal{N}(10, 0.1)$\n",
    "- Condition `1`: generates $\\mathcal{N}(20, 0.2)$\n",
    "\n",
    "A solution to this exersice can be found in the notebook: [BONUS - Conditional GAN from scratch](BONUS - Conditional GAN from scratch.ipynb).\n",
    "\n",
    "\n",
    "---\n",
    "<a id=\"1\">[1]</a>: https://www.tensorflow.org/guide/estimators\n",
    "\n",
    "<a id=\"2\">[2]</a>: https://www.tensorflow.org/guide/datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "330px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
