{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN in Theory and Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Theory\" data-toc-modified-id=\"Theory-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Theory</a></span><ul class=\"toc-item\"><li><span><a href=\"#Enter-DCGAN\" data-toc-modified-id=\"Enter-DCGAN-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Enter DCGAN</a></span></li><li><span><a href=\"#Generator:-from-noise-to-insight\" data-toc-modified-id=\"Generator:-from-noise-to-insight-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Generator: from noise to insight</a></span><ul class=\"toc-item\"><li><span><a href=\"#Deconvolution\" data-toc-modified-id=\"Deconvolution-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Deconvolution</a></span></li><li><span><a href=\"#Batch-Normalization\" data-toc-modified-id=\"Batch-Normalization-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Batch Normalization</a></span></li><li><span><a href=\"#generator_fn()\" data-toc-modified-id=\"generator_fn()-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>generator_fn()</a></span></li></ul></li><li><span><a href=\"#Discriminator\" data-toc-modified-id=\"Discriminator-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Discriminator</a></span><ul class=\"toc-item\"><li><span><a href=\"#discriminator_fn()\" data-toc-modified-id=\"discriminator_fn()-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>discriminator_fn()</a></span></li></ul></li><li><span><a href=\"#Loss-function:-a-bridge-between-two-networks\" data-toc-modified-id=\"Loss-function:-a-bridge-between-two-networks-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Loss function: a bridge between two networks</a></span></li></ul></li><li><span><a href=\"#Practice---Introduction\" data-toc-modified-id=\"Practice---Introduction-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Practice - Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Input-Functions\" data-toc-modified-id=\"Input-Functions-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Input Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-Images\" data-toc-modified-id=\"Loading-Images-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Loading Images</a></span></li></ul></li><li><span><a href=\"#Hyperparameters\" data-toc-modified-id=\"Hyperparameters-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Hyperparameters</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#TensorBoard\" data-toc-modified-id=\"TensorBoard-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>TensorBoard</a></span></li></ul></li><li><span><a href=\"#Predictions\" data-toc-modified-id=\"Predictions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Predictions</a></span><ul class=\"toc-item\"><li><span><a href=\"#predict_input_fn()\" data-toc-modified-id=\"predict_input_fn()-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>predict_input_fn()</a></span></li></ul></li><li><span><a href=\"#Preparing-for-Production\" data-toc-modified-id=\"Preparing-for-Production-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Preparing for Production</a></span><ul class=\"toc-item\"><li><span><a href=\"#serving_input_receiver_fn\" data-toc-modified-id=\"serving_input_receiver_fn-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>serving_input_receiver_fn</a></span></li><li><span><a href=\"#Exporting-the-model-for-production\" data-toc-modified-id=\"Exporting-the-model-for-production-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Exporting the model for production</a></span></li></ul></li><li><span><a href=\"#To-the-serving\" data-toc-modified-id=\"To-the-serving-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>To the serving</a></span></li><li><span><a href=\"#NOTES\" data-toc-modified-id=\"NOTES-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>NOTES</a></span></li><li><span><a href=\"#Links\" data-toc-modified-id=\"Links-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Links</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tfgan = tf.contrib.gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DCGAN](images/dcgan.png)\n",
    "\n",
    "If you take a look at an [impressive list of GANs](), you see that DCGAN, the architecture of our choice, is just a small drop in the ocean. While there may be sexier ones, very few offers the same package of clarity, stability, performances and computational efficiency, these are the main one reason why DCGAN is considered one of the cornerstones of this field.\n",
    "\n",
    "Proposed in a 2015 paper by Alec Radford, Luke Metz, Soumith Chintala, the idea behind DCGAN is quite straightforward: combining a set of architectural constraints with the power of CNN yields us a robust, stable, and competitive model. Succeeding where many others failed, the authors present us with a simple model, asymmetrical architecture, 4 deconvolutional layers at the Generator with 4 layers of convolution at the Discriminator with the following constraints:\n",
    "\n",
    "- All pooling layers are replaced with stranded convolutions (discriminator) and fractional-stride convolutions (generator).\n",
    "- Batch-normalization used in both networks.\n",
    "- Removal of fully connected layers (except for the discriminator output)\n",
    "- `ReLU` for all Generator layers except the output, which uses `tanh`.\n",
    "- `LeakyReLU` activation in the discriminator for all layers.\n",
    "\n",
    "While there are have been very recent advancements in state of the art (i.e., CoordConv, Spectral Normalization), we felt that before venturing into the bleediest of the edges it is essential to have a firm understanding of the basic concept. We leave you (an opinionated) list of further resources you can use to get up to speed to the most exciting line of research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator: from noise to insight\n",
    "\n",
    "![DCGAN Generator](images/dcgan_generator.png)\n",
    "\n",
    "If you recall the theoretical explanation, the Generator is the network responsible for the data-generation, learning how to fool its discriminator allows it to produce realistic results \"sampled' from the learned manifold.\n",
    "\n",
    "The most common type of generator_inputs is pure and straightforward noise; however, more specialized GANs may require additional parameters. Since our full-demo uses a Deep Convolutional GAN (DCGAN), we don't need any other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deconvolution\n",
    "\n",
    "> When we have neural networks generate images, we often have them build them up from low resolution, high-level \n",
    "> descriptions. This allows the network to describe the rough image and then fill in the details.\n",
    ">\n",
    "> In order to do this, we need some way to go from a lower resolution image to a higher one. We generally do this with the deconvolution operation. \n",
    "> Roughly, deconvolution layers allow the model to use every point in the small image to “paint” a square in the larger one.\n",
    "\n",
    "We have implemented the deconvolution layers following the findings explained [here]().\n",
    "\n",
    "The core idea behind the paper is that boundaries and checkerboards artifacts are not due the adversarial training but are inherent to the deconvolution process, to tackle this issue, the suggested solution is an improved version of the upsampling operation, called the **different resize-convolution** whose core is a resize operation powered by either **nearest-neighbor interpolation** or **bilinear interpolation**. \n",
    "\n",
    "As in the paper, we are going with the ** nearest-neighbor** one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv2d(inputs, filters, strides=(1, 1), activation=tf.nn.relu):\n",
    "    \"\"\"\\\"Deconvolution\\\" layer.\n",
    "    \n",
    "    It uses upsampling with nearest neighbor interpolation to reduce the\n",
    "    presence of checkboard artifacts.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_h, input_w = inputs.shape[1].value, inputs.shape[2].value\n",
    "    layer_1 = tf.image.resize_nearest_neighbor(\n",
    "        inputs, (2 * input_h, 2 * input_w), name=\"NNUpSample2D\"\n",
    "    )\n",
    "    # Padding before convolution is used to reduce boundary artifacts\n",
    "    layer_1 = tf.pad(layer_1, [[0, 0], [2, 2], [2, 2], [0, 0]], mode=\"CONSTANT\")\n",
    "    layer_2 = tf.layers.conv2d(\n",
    "        inputs=layer_1,\n",
    "        filters=filters,\n",
    "        kernel_size=5,\n",
    "        padding=\"valid\",\n",
    "        use_bias=False,\n",
    "        activation=activation,\n",
    "        strides=strides,\n",
    "    )\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization\n",
    "\n",
    "We do not enter into the theoretical depth of `BatchNorm` as the discussion on why or whether it works it's still open and going [NOTE 1](). What you need to know is that TensorFlow makes it very easy to implement such an operation.\n",
    "\n",
    "```python\n",
    "tf.layers.batch_normalization(input, training=is_training)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generator_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_fn(inputs, mode):\n",
    "    \"\"\"Generator producing images from noise.\n",
    "        Args:\n",
    "            noise: A single Tensor representing noise.\n",
    "            mode: Either TRAIN, EVAL or PREDICT\n",
    "        Returns:\n",
    "            A 64x64 (None, 4096) flattened tensor whose values are\n",
    "            inside the (-1, 1) range.\"\"\"\n",
    "    is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    linear = tf.layers.dense(inputs=inputs, units=1024 * 4 * 4, activation=tf.nn.relu)\n",
    "    net = tf.reshape(linear, (-1, 4, 4, 1024))\n",
    "    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "    net = deconv2d(net, 512)\n",
    "    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "    net = deconv2d(net, 256)\n",
    "    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "    net = deconv2d(net, 128)\n",
    "    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "    net = deconv2d(net, 64)\n",
    "    net = tf.layers.conv2d(\n",
    "        inputs=net,\n",
    "        filters=3,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        data_format=\"channels_last\",\n",
    "        use_bias=False,\n",
    "        strides=(1, 1),)\n",
    "    output = tf.tanh(net)\n",
    "    print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "![DCGAN Discriminator](images/dcgan_discriminator.png)\n",
    "\n",
    "As for the `input_fn()` we can reuse all the code we defined for the vanilla `Estimator` describing the discriminator network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### discriminator_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_conv2d(inputs, filters, strides=(2,2)):\n",
    "    \"\"\"Helper layer used to instatiate `tf.layers.conv2d` with proper arguments.\"\"\"\n",
    "    layer_1 = tf.layers.conv2d(\n",
    "        inputs=inputs,\n",
    "        filters=filters,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        data_format=\"channels_last\",\n",
    "        use_bias=False,\n",
    "        strides=strides,\n",
    "    )\n",
    "    layer_1 = tf.nn.leaky_relu(layer_1, alpha=0.2)\n",
    "\n",
    "    return layer_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_fn(inputs, conditioning, mode):\n",
    "    \"\"\"Build the Discriminator network.\n",
    "    Args:\n",
    "        features: a batch of images to classify, expected input shape (None, 64, 64 , 3)\n",
    "        labels: a batch of labels\n",
    "        mode: the tf.estimator.ModeKey\n",
    "        params: a dict of optional parameters\n",
    "    Returns:\n",
    "            The output (logits) of the discriminator.\n",
    "    \"\"\"\n",
    "    \n",
    "    # In every mode, define the model\n",
    "    is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    net = custom_conv2d(inputs, filters=64)\n",
    "    net = custom_conv2d(inputs, filters=128)\n",
    "    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "    net = custom_conv2d(net, filters=256)\n",
    "    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "    net = custom_conv2d(net, filters=512)\n",
    "    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "    net = custom_conv2d(net, filters=1024)\n",
    "    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "    net = tf.reshape(net, (-1, net.shape[1] * net.shape[2] * net.shape[3]))\n",
    "    net = tf.layers.dense(net, units=1)\n",
    "    out = tf.identity(net)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function: a bridge between two networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of loss functions usable in GANs'architectures from very domain-specific ones to ones that are perfect for general use cases. Since our goal here falls in the latter category, we use the so-called **Non-Saturating Loss** which is the non-saturating variant of the **MinMax Loss** proposed by Goodfellow in the [original paper]().\n",
    "\n",
    "As stated earlier, one of the beauties of `TFGAN` is its offerings losses ready for use, if you cannot find the loss you want, you can create your own.\n",
    "\n",
    "We use the following two losses:\n",
    "\n",
    "```python\n",
    "generator_loss_fn=tfgan.losses.minimax_generator_loss\n",
    "\n",
    "discriminator_loss_fn=tfgan.losses.minimax_discriminator_loss\n",
    "```\n",
    "\n",
    "It is that easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice - Introduction\n",
    "\n",
    "Having seen the general structure of the TFGAN library, we dive right into the model architecture.\n",
    "\n",
    "In our showcasing of the TensorFlow API, we have built an image recognition network, GANs, however, require the training of both a Generator and a Discriminator together. While this task is either verbose and possibly performance-constrained (in the case of writing it in vanilla TensorFlow) or complicated (Estimator API) thanks to GANEstimator it becomes remarkably simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Functions\n",
    "\n",
    "The input function for the `GANEstimator` is very much the same as the one we would define for a normal `tf.Estimator`.\n",
    "\n",
    "\n",
    ">    The function should construct and return one of the following:\n",
    ">\n",
    ">    A tf.data.Dataset object: Outputs of Dataset object must be a tuple (features, labels) with same constraints as below.\n",
    ">\n",
    ">    A tuple (features, labels): Where `features` is a Tensor or a dictionary of string feature name to Tensor and >`labels` is a Tensor or a dictionary of string label name to Tensor. Model_fn consumes both features and labels. They should satisfy the expectation of model_fn from inputs.\n",
    "\n",
    "\n",
    "We write our input function following the style and structure presented available on [TensorFlow Models](#1) as an example.\n",
    "\n",
    "> ```python\n",
    "def _get_train_input_fn(batch_size, noise_dims, dataset_dir=None,\n",
    "                        num_threads=4):\n",
    "  def train_input_fn():\n",
    "    with tf.device('/cpu:0'):\n",
    "      images, _, _ = data_provider.provide_data(\n",
    "          'train', batch_size, dataset_dir, num_threads=num_threads)\n",
    "    noise = tf.random_normal([batch_size, noise_dims])\n",
    "    return noise, images\n",
    "return train_input_fn\n",
    ">```\n",
    "\n",
    "The two main ideas to take from this are: \n",
    "\n",
    "1. structure the code in the same way, the `GANEstimator` wants **function objects** as inputs thus the need to wrap them. \n",
    "2. Return **noise** first and then the **real_data** (here called `images`) and optionally **labels**.\n",
    "3. **EXP:** while often you want to work with **features (and labels)** fetched from a `tf.Dataset` [see REFERENCE](), the **noise** should always be instantiated using a simple TensorFlow node. Trying to create a noise-containing `tf.Dataset` for the `train_input_fn` is not worth the effort. NOTE: that as [we will see later](), things are different for the`_get_predict_input_fn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Images\n",
    "\n",
    "We reuse the same `input_fn()` we defined earlier with minor changes in order to make it consistent with the aforementioned code strfucture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_train_images_input_fn(file_pattern, image_size=(64, 64, 3), shuffle=False,\n",
    "                 batch_size=32, num_epochs=None, buffer_size=4096):\n",
    "    \"\"\"get_input_fn exploits the `file_pattern` to create an input_fn that reads all the content\n",
    "    of the specified pattern, creating an object dataset.\n",
    "    \n",
    "    Args:\n",
    "        file_pattern: python string, the pattern of the file to read to generate the dataset\n",
    "        image_size: the new size of the read images\n",
    "        shuffle: True if the order of the elements in the generated dataset shold be randomized\n",
    "        batch_size: the size of the batches\n",
    "        num_epochs: the number of epochs to repeat the dataset before throwing an exeption; None is unlimited\n",
    "        buffer_size: how many images read before starting to generate output\n",
    "    Returns:\n",
    "        input_fn: the generated input_fn that returns a correctly instantiated iterator\n",
    "    \"\"\"\n",
    "    \n",
    "    def _img_string_to_tensor(image_string):\n",
    "        \"\"\"Decode an image as read from a `tf.decode_raw`, scales it between 0-1 and resize the\n",
    "        image as specified in the parent method.\n",
    "        Args:\n",
    "            image_string: the raw image tensor\n",
    "        Returns:\n",
    "            image_resize: image in [0,1] correctly resized\n",
    "        \"\"\"\n",
    "        \n",
    "        nonlocal image_size\n",
    "        \n",
    "        image_decoded = tf.image.decode_jpeg(image_string, channels=image_size[-1])\n",
    "        # The conversion to float automatically scales the values in [0., 1.]\n",
    "        image_decoded_as_float = tf.image.convert_image_dtype(image_decoded, dtype=tf.float32)\n",
    "        image_decoded = (image_decoded_as_float - 0.5) * 2\n",
    "        image_resized = tf.image.resize_images(image_decoded, size=image_size[:2])\n",
    "        \n",
    "\n",
    "        return image_resized\n",
    "\n",
    "    def _path_to_img(path):\n",
    "        \"\"\"Given the path of an image, returns the pair (image, label)\n",
    "        where image is the corretly resized image, and label is the label associated with it.\n",
    "        Args:\n",
    "            path: the path of the image to read\n",
    "        Returns:\n",
    "            (image_resized, label): the image, label pair associated the path\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the parent folder of this file to get its class\n",
    "        # Associate the label 0 to dogs and 1 to cats\n",
    "        label = tf.cond(\n",
    "                    tf.equal(tf.string_split([path], delimiter='/').values[-2], \"dogs\"),\n",
    "                    lambda: 0, lambda: 1)\n",
    "\n",
    "        image_string = tf.read_file(path) # read image and process it\n",
    "        image_resized = _img_string_to_tensor(image_string)\n",
    "\n",
    "        return image_resized, label\n",
    "    \n",
    "    def _input_fn():\n",
    "        \"\"\"The input function that builds the `tf.data.Dataset` object and instantiate\n",
    "        the iterator correctly ready to be use.\n",
    "        Returns:\n",
    "            the iterator associated to the built Dataset object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Use the static method `list_files` that builds a dataset of all\n",
    "        # files matching this pattern.\n",
    "        dataset_path = tf.data.Dataset.list_files(file_pattern)\n",
    "\n",
    "        if shuffle:\n",
    "            dataset_path = dataset_path.apply(tf.contrib.data.shuffle_and_repeat(buffer_size, num_epochs))\n",
    "        else:\n",
    "            dataset_path = dataset_path.repeat(num_epochs)\n",
    "\n",
    "        # The map function maps the path to the pair (image, label)\n",
    "        dataset = dataset_path.map(_path_to_img)\n",
    "        dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\n",
    "        dataset = dataset.prefetch(buffer_size)\n",
    "        \n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        return iterator.get_next()\n",
    "\n",
    "    return _input_fn()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the `_get_train_input_fn()` which return a Python Callable that we instantiate and pass to `GANEstimator.train()` later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_train_input_fn(file_pattern, batch_size, num_epochs, noise_dims=100, **kwargs):\n",
    "    def train_input_fn():\n",
    "        real_data = _get_train_images_input_fn(\n",
    "                 file_pattern,\n",
    "                 batch_size=batch_size, \n",
    "                 num_epochs=num_epochs)\n",
    "        noise = tf.random_normal([batch_size, noise_dims], name=\"train_noise\")\n",
    "        real_data.set_shape((batch_size,) + tuple(real_data.shape[1:]))\n",
    "        print(noise, real_data)\n",
    "        return noise, real_data\n",
    "    return train_input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personally I like to keep my hyperparameters in a **TOML** file, loading them before the train.\n",
    "\n",
    "Here we simply load them in a `Dict` and use `**` to unpack them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hyperparameters():\n",
    "    hp = {\n",
    "        \"model_dir\": \"../logs/\",\n",
    "        \"file_pattern\": \"../assets/celeba/*.png\",\n",
    "        \"batch_size\": 128,\n",
    "        \"num_epochs\":1,\n",
    "        \"noise_dims\": 100\n",
    "    }\n",
    "    return hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcgan():\n",
    "    \n",
    "    # Set the seed at a Graph Level so that we geet consistency between runs.\n",
    "    tf.set_random_seed(42)\n",
    "    \n",
    "    # Hyperparameters\n",
    "    hyperparameters = load_hyperparameters()\n",
    "    \n",
    "    # Run Configuration (it has other arguments)\n",
    "    run_config = tf.estimator.RunConfig(\n",
    "        model_dir=hyperparameters.get(\"model_dir\"), save_summary_steps=50, save_checkpoints_steps=500)\n",
    "    \n",
    "    # Instatiate the GANEstimator object\n",
    "    gan_estimator = tfgan.estimator.GANEstimator(\n",
    "        config=run_config,\n",
    "        generator_fn=generator_fn,\n",
    "        discriminator_fn=discriminator_fn,\n",
    "        generator_loss_fn=tfgan.losses.modified_generator_loss,\n",
    "        discriminator_loss_fn=tfgan.losses.modified_discriminator_loss,\n",
    "        generator_optimizer=tf.train.AdamOptimizer(0.0002, 0.5),\n",
    "        discriminator_optimizer=tf.train.AdamOptimizer(0.0002, 0.5),\n",
    "        add_summaries=tfgan.estimator.SummaryType.IMAGES\n",
    "    )\n",
    "    \n",
    "    # Instatiate the train_input_fn\n",
    "    # The model will train until it exhausts the Dataset which is repeated EPOCH times\n",
    "    train_input_fn = _get_train_input_fn(**hyperparameters)\n",
    "    trained_model = gan_estimator.train(train_input_fn, max_steps=None)\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "\n",
    "In order to track our training we need to launch a **TensorBoard** session pointing to the folder (`model_dir`) containing the logs generated by our training.\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=PATH_TO_YOUR_MODEL_DIR\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '../logs/', '_tf_random_seed': None, '_save_summary_steps': 50, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb99ba7a240>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From <ipython-input-6-e678ba1cfba7>:75: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n",
      "Tensor(\"train_noise:0\", shape=(128, 100), dtype=float32, device=/device:CPU:0) Tensor(\"IteratorGetNext:0\", shape=(128, 64, 64, 3), dtype=float32, device=/device:CPU:0)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Tensor(\"Generator/Tanh:0\", shape=(128, 64, 64, 3), dtype=float32)\n",
      "INFO:tensorflow:Summary name Generator/dense/kernel:0 is illegal; using Generator/dense/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/dense/bias:0 is illegal; using Generator/dense/bias_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/batch_normalization/gamma:0 is illegal; using Generator/batch_normalization/gamma_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/batch_normalization/beta:0 is illegal; using Generator/batch_normalization/beta_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/conv2d/kernel:0 is illegal; using Generator/conv2d/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/batch_normalization_1/gamma:0 is illegal; using Generator/batch_normalization_1/gamma_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/batch_normalization_1/beta:0 is illegal; using Generator/batch_normalization_1/beta_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/conv2d_1/kernel:0 is illegal; using Generator/conv2d_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/batch_normalization_2/gamma:0 is illegal; using Generator/batch_normalization_2/gamma_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/batch_normalization_2/beta:0 is illegal; using Generator/batch_normalization_2/beta_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/conv2d_2/kernel:0 is illegal; using Generator/conv2d_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/batch_normalization_3/gamma:0 is illegal; using Generator/batch_normalization_3/gamma_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/batch_normalization_3/beta:0 is illegal; using Generator/batch_normalization_3/beta_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/conv2d_3/kernel:0 is illegal; using Generator/conv2d_3/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name Generator/conv2d_4/kernel:0 is illegal; using Generator/conv2d_4/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/conv2d/kernel:0 is illegal; using Discriminator/conv2d/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/conv2d_1/kernel:0 is illegal; using Discriminator/conv2d_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/batch_normalization/gamma:0 is illegal; using Discriminator/batch_normalization/gamma_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/batch_normalization/beta:0 is illegal; using Discriminator/batch_normalization/beta_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/conv2d_2/kernel:0 is illegal; using Discriminator/conv2d_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/batch_normalization_1/gamma:0 is illegal; using Discriminator/batch_normalization_1/gamma_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/batch_normalization_1/beta:0 is illegal; using Discriminator/batch_normalization_1/beta_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/conv2d_3/kernel:0 is illegal; using Discriminator/conv2d_3/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/batch_normalization_2/gamma:0 is illegal; using Discriminator/batch_normalization_2/gamma_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/batch_normalization_2/beta:0 is illegal; using Discriminator/batch_normalization_2/beta_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/conv2d_4/kernel:0 is illegal; using Discriminator/conv2d_4/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/batch_normalization_3/gamma:0 is illegal; using Discriminator/batch_normalization_3/gamma_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/batch_normalization_3/beta:0 is illegal; using Discriminator/batch_normalization_3/beta_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/dense/kernel:0 is illegal; using Discriminator/dense/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name Discriminator/dense/bias:0 is illegal; using Discriminator/dense/bias_0 instead.\n",
      "WARNING:tensorflow:update_ops in create_train_op does not contain all the  update_ops in GraphKeys.UPDATE_OPS\n",
      "WARNING:tensorflow:update_ops in create_train_op does not contain all the  update_ops in GraphKeys.UPDATE_OPS\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../logs/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into ../logs/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: None.\n"
     ]
    }
   ],
   "source": [
    "trained_model = dcgan()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "We train Machine Learning models because we want them to perform some specific task since we now have our GAN trained, we can finally use it to \"predict\" AKA generate a new image from a noise vector.\n",
    "\n",
    "Once again, TFGAN makes it as easy as invoking the `predict()`  method of our `GANEstimator` while passing to it a `predict_input_fn` as a required argument. \n",
    "\n",
    "### predict_input_fn()\n",
    "\n",
    "As previously mentioned while theoretically identical, `train_input_fn` and `predict_nput_fn()` should be implemented differently.  The first one is a simple `tf.random_normal()` node, the second should make proper use of the `tf.Dataset` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict_input_fn(batch_size, noise_dims=100, **kwargs):\n",
    "    \n",
    "    def predict_input_fn():\n",
    "        noise_gen = np.array([np.float32(np.random.normal(size=[1, noise_dims])) for i in range(batch_size)])\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(noise_gen)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        return iterator.get_next()\n",
    "        \n",
    "    return predict_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "Tensor(\"Generator/Tanh:0\", shape=(?, 64, 64, 3), dtype=float32)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../logs/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[[ 0.41792387,  0.61920786,  0.50794834],\n",
       "         [ 0.62908447,  0.666271  ,  0.59738606],\n",
       "         [ 0.7904184 ,  0.7635082 ,  0.6563889 ],\n",
       "         ...,\n",
       "         [ 1.        ,  1.        ,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        ],\n",
       "         [ 1.        ,  0.9999996 ,  0.9999995 ]],\n",
       " \n",
       "        [[ 0.5218568 ,  0.59874445,  0.5349896 ],\n",
       "         [ 0.60220337,  0.67490005,  0.5795506 ],\n",
       "         [ 0.74352103,  0.754142  ,  0.64505184],\n",
       "         ...,\n",
       "         [ 1.        ,  1.        ,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        ]],\n",
       " \n",
       "        [[ 0.5708438 ,  0.6581632 ,  0.5898274 ],\n",
       "         [ 0.64129364,  0.6874107 ,  0.6413022 ],\n",
       "         [ 0.74881834,  0.77094173,  0.72677755],\n",
       "         ...,\n",
       "         [ 1.        ,  1.        ,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        ],\n",
       "         [ 1.        ,  1.        ,  1.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.04962189,  0.06502746,  0.07363978],\n",
       "         [ 0.08840063,  0.12777251,  0.1128386 ],\n",
       "         [ 0.14527051,  0.14914599,  0.13862115],\n",
       "         ...,\n",
       "         [-0.1378038 , -0.38775122, -0.8557189 ],\n",
       "         [-0.03745811, -0.16721947, -0.8230906 ],\n",
       "         [-0.16064075, -0.3595631 , -0.79399824]],\n",
       " \n",
       "        [[ 0.1200036 ,  0.14283518,  0.111773  ],\n",
       "         [ 0.11670331,  0.12600349,  0.10263027],\n",
       "         [ 0.14765777,  0.19206455,  0.1973058 ],\n",
       "         ...,\n",
       "         [ 0.18292397,  0.12918843, -0.74259925],\n",
       "         [ 0.4962308 ,  0.41858652, -0.46562618],\n",
       "         [ 0.11640583,  0.05074443, -0.6431116 ]],\n",
       " \n",
       "        [[ 0.16185288,  0.16624801,  0.14882733],\n",
       "         [ 0.10211714,  0.13515079,  0.12683626],\n",
       "         [ 0.09477688,  0.15385184,  0.16212417],\n",
       "         ...,\n",
       "         [-0.0751237 ,  0.02774782, -0.8284249 ],\n",
       "         [-0.11000276,  0.05950584, -0.79088473],\n",
       "         [-0.48974094, -0.3279681 , -0.8226392 ]]], dtype=float32),\n",
       " array([[[-2.36377552e-01, -3.26257914e-01, -5.57996988e-01],\n",
       "         [-1.42403603e-01, -2.40959600e-01, -4.58102584e-01],\n",
       "         [-1.76660076e-01, -3.87124181e-01, -5.63192844e-01],\n",
       "         ...,\n",
       "         [-8.82481098e-01, -9.07215416e-01, -9.48743224e-01],\n",
       "         [-8.80623639e-01, -8.90389621e-01, -9.35802937e-01],\n",
       "         [-8.37013006e-01, -8.73420000e-01, -9.03181374e-01]],\n",
       " \n",
       "        [[-7.33808875e-02, -1.55585244e-01, -4.59506452e-01],\n",
       "         [ 1.36337511e-03, -8.47409889e-02, -3.98976654e-01],\n",
       "         [ 2.04617418e-02, -1.45906731e-01, -3.96357328e-01],\n",
       "         ...,\n",
       "         [-7.87690818e-01, -8.61664772e-01, -9.33291197e-01],\n",
       "         [-7.59130478e-01, -8.33559453e-01, -9.34109986e-01],\n",
       "         [-8.11022341e-01, -8.64432991e-01, -9.29569066e-01]],\n",
       " \n",
       "        [[ 1.38808750e-02, -5.32420934e-04, -3.48026216e-01],\n",
       "         [-3.47427949e-02, -1.18805818e-01, -3.65509003e-01],\n",
       "         [ 9.74990353e-02, -2.53817104e-02, -2.91975170e-01],\n",
       "         ...,\n",
       "         [-7.01545238e-01, -7.86469579e-01, -8.97140801e-01],\n",
       "         [-7.60286927e-01, -8.23690772e-01, -9.29545403e-01],\n",
       "         [-7.63199568e-01, -8.11719954e-01, -9.14358854e-01]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 8.49594027e-02,  6.39237240e-02, -4.94862273e-02],\n",
       "         [ 7.68558607e-02,  5.12138046e-02, -5.87073229e-02],\n",
       "         [ 9.66996849e-02,  2.79780533e-02, -8.50436389e-02],\n",
       "         ...,\n",
       "         [ 5.31616509e-02,  8.30257684e-02, -9.56941247e-02],\n",
       "         [ 1.94971576e-01,  2.13569358e-01,  5.07564731e-02],\n",
       "         [ 1.49086565e-01,  1.80537552e-01,  1.43596735e-02]],\n",
       " \n",
       "        [[ 1.35563314e-01,  1.33048147e-01, -6.15890287e-02],\n",
       "         [ 1.59145176e-01,  9.66676697e-02, -3.25137489e-02],\n",
       "         [ 1.12842709e-01,  5.21855913e-02, -4.78582829e-02],\n",
       "         ...,\n",
       "         [ 1.44005239e-01,  2.69719511e-01,  6.05119206e-02],\n",
       "         [ 2.89685786e-01,  3.52959901e-01,  2.18866542e-01],\n",
       "         [ 1.77056879e-01,  2.73583680e-01,  9.18992981e-02]],\n",
       " \n",
       "        [[ 2.13035688e-01,  1.80185735e-01, -2.98893283e-04],\n",
       "         [ 1.85325250e-01,  1.42804757e-01, -9.56288353e-03],\n",
       "         [ 1.23984762e-01,  1.05843924e-01, -7.45496713e-03],\n",
       "         ...,\n",
       "         [ 1.61949426e-01,  2.76281804e-01,  1.05405569e-01],\n",
       "         [ 2.27503270e-01,  3.20556730e-01,  1.06165551e-01],\n",
       "         [ 1.10312089e-01,  2.11526528e-01,  7.06320703e-02]]],\n",
       "       dtype=float32),\n",
       " array([[[-1.1457657e-01,  1.1262893e-01,  1.9813053e-01],\n",
       "         [-5.3763904e-02,  1.3536622e-01,  2.4453986e-01],\n",
       "         [-1.2487019e-01,  6.0874928e-04,  1.2938012e-01],\n",
       "         ...,\n",
       "         [-6.6455096e-01, -4.4220546e-01, -1.6455758e-01],\n",
       "         [-6.4629567e-01, -3.7117311e-01, -1.4772155e-02],\n",
       "         [-5.6155920e-01, -3.0179083e-01,  1.8557733e-02]],\n",
       " \n",
       "        [[ 6.6689201e-02,  2.5200671e-01,  2.9763547e-01],\n",
       "         [ 7.8640647e-02,  2.9063052e-01,  4.0331766e-01],\n",
       "         [-2.0484099e-02,  1.6972119e-01,  2.3525557e-01],\n",
       "         ...,\n",
       "         [-6.1174726e-01, -3.8827160e-01, -1.7978485e-01],\n",
       "         [-5.4529917e-01, -1.5326692e-01,  1.4678587e-01],\n",
       "         [-5.8601451e-01, -2.9610923e-01,  2.1992501e-02]],\n",
       " \n",
       "        [[-2.4452956e-01, -9.8150447e-03,  4.8101489e-02],\n",
       "         [-2.7003786e-01, -6.6903569e-02,  4.6821795e-02],\n",
       "         [-1.6246875e-01,  3.4924541e-02,  1.2792325e-01],\n",
       "         ...,\n",
       "         [-5.0289810e-01, -1.0970998e-01,  6.9148786e-02],\n",
       "         [-4.8827738e-01, -4.0322553e-02,  2.2999531e-01],\n",
       "         [-4.1710618e-01,  1.0615152e-02,  2.2616811e-01]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-5.8070517e-01, -5.1414913e-01, -5.6141138e-01],\n",
       "         [-6.1343277e-01, -5.9349954e-01, -5.9511495e-01],\n",
       "         [-6.9151342e-01, -6.7319155e-01, -7.0419657e-01],\n",
       "         ...,\n",
       "         [ 6.5977311e-01,  7.7310729e-01,  8.9751410e-01],\n",
       "         [ 8.9426374e-01,  9.5577854e-01,  9.8019254e-01],\n",
       "         [ 8.6038387e-01,  9.3246889e-01,  9.6659100e-01]],\n",
       " \n",
       "        [[-6.2073106e-01, -5.4464304e-01, -5.2209854e-01],\n",
       "         [-5.6547731e-01, -5.2201283e-01, -5.4283446e-01],\n",
       "         [-5.2686888e-01, -5.2862525e-01, -6.3178259e-01],\n",
       "         ...,\n",
       "         [ 9.0766340e-01,  9.6852702e-01,  9.8482066e-01],\n",
       "         [ 7.5702989e-01,  9.1742527e-01,  9.6311092e-01],\n",
       "         [ 8.4994161e-01,  9.5780927e-01,  9.7884107e-01]],\n",
       " \n",
       "        [[-5.2851236e-01, -4.1894835e-01, -3.7318647e-01],\n",
       "         [-5.1145881e-01, -3.6859539e-01, -3.6575922e-01],\n",
       "         [-6.1825359e-01, -5.8978099e-01, -6.6707367e-01],\n",
       "         ...,\n",
       "         [ 7.2510600e-01,  9.2828459e-01,  9.6112549e-01],\n",
       "         [ 8.2733840e-01,  9.6115464e-01,  9.7660518e-01],\n",
       "         [ 6.6797405e-01,  9.1808569e-01,  9.5857066e-01]]], dtype=float32)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_batch = 3\n",
    "predict_input_fn = _predict_input_fn(batch_size=predictions_batch)\n",
    "predictions = trained_model.predict(predict_input_fn)\n",
    "[next(predictions) for _ in range(predictions_batch)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for Production\n",
    "\n",
    "Now that we have our model trained and ready we are just a couple steps away from being able to put our model into production.\n",
    "\n",
    "### serving_input_receiver_fn\n",
    "\n",
    "To serve a model in production, we first need to equip it with an interface which will receive data from our requests, such interface is specified by the aptly named `serving_input_receiver_fn`. Of the three input functions, this is the trickiest one since it has its peculiar API.\n",
    "\n",
    "This functions requires its output to be a either a `ServingInputReceiver` or a `TensorServingInputReceiver` object; the documentation on their use is clear:\n",
    "\n",
    "> The normal `ServingInputReceiver` always returns a feature dict, even if it contains only one entry, and so can be used only with models that accept such a dict. \n",
    ">For models that accept only a single raw feature, the `serving_input_receiver_fn` provided to `Estimator.export_savedmodel()` should return this `TensorServingInputReceiver`.\n",
    "\n",
    "Since our model needs only a noise vector to get going, we can use `TensorServingInputReceiver`.\n",
    "\n",
    ">The expected return values are: \n",
    "> - **features**: A single `Tensor` or `SparseTensor`, representing the feature to be passed to the model. \n",
    "> - **receiver_tensors**: A Tensor, SparseTensor, or dict of string to Tensor or SparseTensor, specifying input nodes where this receiver expects to be fed by default. Typically, this is a single placeholder expecting serialized `tf.Example` protos. \n",
    "> - **receiver_tensors_alternatives**: a dict of string to additional groups of receiver tensors, each of which may be a `Tensor`, `SparseTensor`, or dict of string to `Tensor` or `SparseTensor`. These named receiver tensor alternatives generate additional serving signatures, which may be used to feed inputs at different points within the input receiver subgraph. A typical usage is to allow feeding raw feature Tensors downstream of the `tf.parse_example()` op. Defaults to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _serving_input_receiver_fn():\n",
    "    \"\"\"Instantiate a placeholder for our serving input data.\n",
    "\n",
    "    Call the custom function `serving_input_fn()`, built following\n",
    "    TensorFlow Estimator API convention, initializing the placeholder for\n",
    "    the noise we will feed the model during its serving.\n",
    "\n",
    "    The Serving Input function has two key elements:\n",
    "\n",
    "        - the data-processing step, where we concretely prepare data to be\n",
    "        fed to the:\n",
    "        - Placeholder, it is the node where the input are fed.\n",
    "\n",
    "    The things to notice is that while using `ServingInputReceiver`\n",
    "    your data processing step should have at its core the parsing of the\n",
    "    tf.Example received.\n",
    "\n",
    "    With `TensorServingInputReceiver` our data won't really be passed by the\n",
    "    request, instead it will have to be generated ''model-side'' inside the\n",
    "    `_serving_input_receiver_fn` itself.\n",
    "\n",
    "    Returns:\n",
    "        tf.estimator.export.TensorServingInputReceiver passing the\n",
    "        placeholder for the noise to it.\n",
    "    \"\"\"\n",
    "\n",
    "    noise = tf.random_normal([1, 100])\n",
    "    receiver_tensors = tf.placeholder(shape=[None, 100], dtype=tf.float32, name=\"serving_noise\")\n",
    "    return tf.estimator.export.TensorServingInputReceiver(noise, receiver_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the model for production\n",
    "\n",
    "Exporting the model is as easy as calling `GANEstimator.export_savedmodel()` which the same as the normal `Estimator`.\n",
    "\n",
    "> This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single MetaGraphDef saved from this session.\n",
    "\n",
    "We have to specify an output folder, the `serving_input_receiver_fn` and that we want to [strip all nodes default values, ensuing forward compatibility](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "Tensor(\"Generator/Tanh:0\", shape=(1, 64, 64, 3), dtype=float32)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from ../logs/model.ckpt-10000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ../assets/exported_models/temp-b'1535016704'/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'../assets/exported_models/1535016704'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_dir_base = \"../assets/exported_models\"\n",
    "trained_model.export_savedmodel(\n",
    "    export_dir_base, _serving_input_receiver_fn, strip_default_attrs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To the serving\n",
    "\n",
    "Now that we have the exported model we are ready to finally serve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES\n",
    "\n",
    "[1]: [The GAN Landscape: Losses, Architectures, Regularization, and Normalization](https://arxiv.org/abs/1807.04720v1), which in our opinion is one of the most thorough scientific studies on GANs out there, suggests that `Spectral Normalization` is the real big deal and that `BatchNorm` actually hurt performance if applied to the Discriminator. We still decided to go with the classic formulation of DCGAN as not to overtax you with theoretical discussions. We leave the implementation of the `Spectral Normalization`(and SNGAN) to you as an exercise. On the theoretical treating of `BatchNorm` we recommend a back to back reading of [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167), [Understanding Batch Normalization](https://arxiv.org/abs/1806.02375v1), [How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)](https://arxiv.org/abs/1805.11604) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links\n",
    "\n",
    "Generative Adversarial Networks : https://arxiv.org/abs/1406.2661\n",
    "\n",
    "really-awesome-gan : https://github.com/nightrome/really-awesome-gan\n",
    "\n",
    "DCGAN : https://arxiv.org/abs/1511.06434\n",
    "\n",
    "Deconvolution and Checkerboard Artifacts : https://distill.pub/2016/deconv-checkerboard/\n",
    "\n",
    "TFGAN MNIST GAN Example : https://github.com/tensorflow/models/tree/master/research/gan/mnist_estimator\n",
    "\n",
    "Estimator API : https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator\n",
    "\n",
    "export_savedmodel : https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_savedmodel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
