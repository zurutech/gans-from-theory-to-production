{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/logo.jpg\" style=\"width:85px;height:85px;float:left\" /><h1 style=\"position:relative;float:left;display:inline\">Writing an GAN from scratch</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://colab.research.google.com/github/zurutech/gans-from-theory-to-production/blob/master/2.%20GANs%20in%20Tensorflow/2.1.%20Writing%20a%20GAN%20from%20scratch.ipynb'>\n",
    "    <img align=\"left\" src='https://cdn-images-1.medium.com/max/800/1*ZpNn76K98snC9vDiIJ6Ldw.jpeg'></img>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Writing-an-GAN-from-scratch\" data-toc-modified-id=\"Writing-an-GAN-from-scratch-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Writing an GAN from scratch</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-does-a-GAN-learn?\" data-toc-modified-id=\"What-does-a-GAN-learn?-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>What does a GAN learn?</a></span></li><li><span><a href=\"#Input-data\" data-toc-modified-id=\"Input-data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Input data</a></span></li><li><span><a href=\"#Generator-and-discriminator-networks:-Keras-functional-API\" data-toc-modified-id=\"Generator-and-discriminator-networks:-Keras-functional-API-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Generator and discriminator networks: Keras functional API</a></span></li><li><span><a href=\"#Define-input-and-instantiate-networks\" data-toc-modified-id=\"Define-input-and-instantiate-networks-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Define input and instantiate networks</a></span></li><li><span><a href=\"#The-loss-function-and-the-training-procedure\" data-toc-modified-id=\"The-loss-function-and-the-training-procedure-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>The loss function and the training procedure</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discriminator-loss-function\" data-toc-modified-id=\"Discriminator-loss-function-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Discriminator loss function</a></span></li><li><span><a href=\"#Generator-loss-function\" data-toc-modified-id=\"Generator-loss-function-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Generator loss function</a></span></li></ul></li><li><span><a href=\"#Gradient-ascent\" data-toc-modified-id=\"Gradient-ascent-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Gradient ascent</a></span><ul class=\"toc-item\"><li><span><a href=\"#Visualize-training\" data-toc-modified-id=\"Visualize-training-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Visualize training</a></span></li><li><span><a href=\"#Advantages-and-disadvantages\" data-toc-modified-id=\"Advantages-and-disadvantages-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;</span>Advantages and disadvantages</a></span></li></ul></li></ul></li><li><span><a href=\"#Bonus-exercise:-converting-it-to-a-Conditional-GAN\" data-toc-modified-id=\"Bonus-exercise:-converting-it-to-a-Conditional-GAN-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Bonus exercise: converting it to a Conditional GAN</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing an GAN from scratch\n",
    "\n",
    "Previous to version 2.0, Tensorflow allowed us to only define computational graphs. We could hence describe both the generator $G$ and the discriminator $D$, just like two interacting subgraphs.\n",
    "\n",
    "Keras changes the way of reasoning, making the model definition more pythonic: before Keras we had to reason thinking about graphs and scopes; using Keras we have to thing about objects and attributes.\n",
    "\n",
    "In Keras a variable lives inside the model, in pure old tensorflow instead, the variable lives in the global graph.\n",
    "\n",
    "The Keras way of defining the Models is the new standard.\n",
    "\n",
    "### What does a GAN learn?\n",
    "\n",
    "The aim of the GAN we are going to implement is to learn a certain **data distribution** present in the training set.\n",
    "\n",
    "The following example will guide you trough the required steps to build a GAN from scratch using Tensorflow while giving you an **intuition** of what a GAN learns.\n",
    "\n",
    "### Input data\n",
    "\n",
    "In order to give a meaningful example, we're going to generate and visualize the distribution that has to be learn.\n",
    "\n",
    "$$ \\mathcal{N}(\\mu = 10, \\sigma = 0.1) $$\n",
    "\n",
    "Our **target** distribution is a Gaussian distribution centered in $10$ a with a standard deviation of $0.1$\n",
    "\n",
    "We're going to use Tensorflow itself to **sample datapoints** from this distribution in order to build our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required inclues to create the GAN and to visualize the data\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 2.0 defauls to eager mode execution, therefore we can just use tensorflow as we are used to use other libraries such as `numpy` to generate a dataset.\n",
    "We can thus define a function that samples from our target distribution of fixed number of data points (2000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 60)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sample_dataset():\n",
    "    dataset_shape = (2000, 1)\n",
    "    return tf.random.normal(mean=10., shape=dataset_shape, stddev=0.1, dtype=tf.float32)\n",
    "\n",
    "counts, bin, ignored = plt.hist(sample_dataset().numpy(), 100)\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1,11])\n",
    "axes.set_ylim([0, 60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator and discriminator networks: Keras functional API\n",
    "\n",
    "The **generator** $G$ network architecture is **completely arbitrary**: practice suggests that a simple layer with just 2 fully connected layers and a single linear layer at the output is enough for learning such a simple task.\n",
    "\n",
    "The **discriminator** $D$ network architecture is **completely arbitrary** too. The only thing that's mandatory is the use of the **linear activation** in the single output neuron.\n",
    "\n",
    "This is due to the fact that we're going to use a tensorflow built-in loss function to train this classifier and this function requires an \"logits\" as input vector, aka a linear output.\n",
    "\n",
    "We use a standard (and powerful) way of defining Keras models: the functional API.\n",
    "In this formulation we can just use the keras layers as functions: they accept an input and produce an output - easy.\n",
    "\n",
    "For a simple model definition like that a functional approach is not strictly required since a simplier interface like `Sequential` could be enough; however, we described the model using the functional approach because we found it more powerful and architecture-change ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(input_shape):\n",
    "    \"\"\"Defines the generator keras.Model.\n",
    "    Args:\n",
    "        input_shape: the desired input shape (e.g.: (latent_space_size))\n",
    "    Returns:\n",
    "        G: The generator model\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    net = tf.keras.layers.Dense(units=64, activation=tf.nn.elu, name=\"fc1\")(inputs)\n",
    "    net = tf.keras.layers.Dense(units=64, activation=tf.nn.elu, name=\"fc2\")(net)\n",
    "    net = tf.keras.layers.Dense(units=1, name=\"G\")(net)\n",
    "    G = tf.keras.Model(inputs=inputs, outputs=net)\n",
    "    return G\n",
    "\n",
    "def disciminator(input_shape):\n",
    "    \"\"\"Defines the discriminator keras.Model.\n",
    "    Args:\n",
    "        input_shape: the desired input shape (e.g.: (the generator output shape))\n",
    "    Returns:\n",
    "        D: the discriminator model\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    net = tf.keras.layers.Dense(units=32, activation=tf.nn.elu, name=\"fc1\")(inputs)\n",
    "    net = tf.keras.layers.Dense(units=1, name=\"D\")(net)\n",
    "    D = tf.keras.Model(inputs=inputs, outputs=net)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input and instantiate networks\n",
    "\n",
    "The two functions just defined allow us to instantiate the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the real input shape, a batch of values sampled from the real data\n",
    "input_shape = (1,)\n",
    "\n",
    "# Define the discriminator model\n",
    "D = disciminator(input_shape)\n",
    "\n",
    "# Arbitrary set the shape of the noise prior\n",
    "latent_space_shape = (100,)\n",
    "# Define the input noise shape and define the generator\n",
    "G = generator(latent_space_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss function and the training procedure\n",
    "\n",
    "As the theory describes, the training procedure is the **alternating** execution of training steps. In the next few lines we're going to implement the **non saturating** version of the value function.\n",
    "\n",
    "As described above, the output layers of the 2 models have **linear output** because we're going to use the `tf.keras.losses.BinaryCrossentropy(from_logits=True)` that accepts logits as input.\n",
    "\n",
    "In practice, the `tf.keras.losses.BinaryCrossentropy(from_logits=True)` computes the binary cross entropy between two distributions:\n",
    "\n",
    "1. the learned distribution, that assigns a probability to a certain class, that's why the function applies the **sigmoid**  $\\sigma$ function to the output neuron, in order to consider the output a probability (range [0-1])\n",
    "2. the conditional empirical distribution over class labels (a probability distribution where the probability of the current observed positive sample is 1 and is 0 for any other class)\n",
    "\n",
    "$$ \\mathcal{L}_{BCE} = y - \\log(\\hat{y}) - (1 - y)\\log(1 - \\hat{y}) $$\n",
    "\n",
    "#### Discriminator loss function\n",
    "\n",
    "Given the labels for the positive class and the nevative class to be 1 and 0 respectively, the loss becomes the sum of 2 BCE:\n",
    "\n",
    "$$ \\frac{1}{m} \\sum_{i=1}^{m}- \\log \\sigma(D(x^{(i)})) + \\frac{1}{m} \\sum_{i=1}^{m} - \\log(1 - \\sigma(D(G(z^{(i)})) $$\n",
    "\n",
    "The loss function is the binary cross entropy between the class of the real samples (label 1) and the class of the generated samples (label 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_loss(real_output, generated_output):\n",
    "    \"\"\"The disciminator loss function.\"\"\"\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    return bce(tf.ones_like(real_output), real_output) + bce(tf.zeros_like(generated_output), generated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator loss function\n",
    "$$ -\\frac{1}{m} \\sum_{i=1}^{m} \\sigma(\\log(D(G(z)))) $$\n",
    "\n",
    "The loss function is just the binary cross entropy between the log probability of the generated images and the distribution of the real images (label 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_loss(generated_output):\n",
    "    \"\"\"The Generator loss function.\"\"\"\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    return bce(tf.ones_like(generated_output), generated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient ascent\n",
    "\n",
    "In order to train 2 different networks one next to the other, we can just exploit the fact that we can specify a the list of variables to train to an optimizer and it will take care of updating only that variables.\n",
    "Using Keras this is for free, since every single Model brings its own set of `trainable_variables`.\n",
    "\n",
    "The training process is **exactly** the one described in the GAN paper thanks to the eager mode.\n",
    "\n",
    "The `@tf.function` annotation allow to convert the most computational-intensive part of the training into its graph representation and accelerate, thus, the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's play the min-max game\n",
    "if not os.path.exists(\"./gif/\"):\n",
    "     os.makedirs(\"./gif/\")\n",
    "\n",
    "def train():\n",
    "    # Define the optimizers and the train operations\n",
    "    optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step():\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            real_data = sample_dataset()\n",
    "            noise_vector = tf.random.normal(mean=0, stddev=1, shape=(real_data.shape[0], latent_space_shape[0]))\n",
    "            # Sample from the Generator\n",
    "            fake_data = G(noise_vector)\n",
    "            # Compute the D loss\n",
    "            d_fake_data = D(fake_data)\n",
    "            d_real_data = D(real_data)\n",
    "            d_loss_value = d_loss(generated_output=d_fake_data, real_output=d_real_data)\n",
    "            # Compute the G loss\n",
    "            g_loss_value = g_loss(generated_output=d_fake_data)\n",
    "        # Now that we comptuted the losses we can compute the gradient (using the tape)\n",
    "        # and optimize the networks\n",
    "        d_gradients = tape.gradient(d_loss_value, D.trainable_variables)\n",
    "        g_gradients = tape.gradient(g_loss_value, G.trainable_variables)\n",
    "        del tape\n",
    "        \n",
    "        optimizer.apply_gradients(zip(d_gradients, D.trainable_variables))\n",
    "        optimizer.apply_gradients(zip(g_gradients, G.trainable_variables))\n",
    "        return real_data, fake_data, g_loss_value, d_loss_value\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    for step in range(40000):\n",
    "        real_data, fake_data,g_loss_value, d_loss_value = train_step()\n",
    "        if step % 200 == 0:\n",
    "            print(\"G loss: \", g_loss_value.numpy(), \" D loss: \", d_loss_value.numpy(), \" step: \", step)\n",
    "\n",
    "            # Sample 5000 values from the Generator and draw the histogram\n",
    "            ax.hist(fake_data.numpy(), 100)\n",
    "            ax.hist(real_data.numpy(), 100)\n",
    "            # these are matplotlib.patch.Patch properties\n",
    "            props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "            # place a text box in upper left in axes coords\n",
    "            textstr = f\"step={step}\"\n",
    "            ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n",
    "                    verticalalignment='top', bbox=props)\n",
    "\n",
    "            axes = plt.gca()\n",
    "            axes.set_xlim([-1,11])\n",
    "            axes.set_ylim([0, 60])\n",
    "            display.display(pl.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            plt.savefig(\"./gif/{}.png\".format(step))\n",
    "            plt.gca().clear()\n",
    "            \n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize training\n",
    "\n",
    "Every 200 steps of train we generated and saved the histogram of 2000 values sampled from the target distribution and 2000 values sampled from the generator output.\n",
    "\n",
    "\n",
    "In order to generate a nice animation of the learning process we can just use the `imagemagick` tool to merge all the images into a pretty cool gif of the learning process.\n",
    "This gif shows you how the learning process learned to shift the initial random distribution to the correct value of $10$ and to adjust the variance in order to be close to the one of **target distribution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! convert -delay 20 -loop 0 $(ls gif/*.png | sort -V) gif/learning_gaussian.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![learn](gif/learning_gaussian.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages and disadvantages\n",
    "\n",
    "Writing a GAN from scratch using the new Tensorflow 2.0 API is extremely helpful for learning how to correctly define our own training loop, how the write a GAN, and understand the theory required to make the network learn.\n",
    "\n",
    "However, this example could become way more complex if we add:\n",
    "\n",
    "- The dataset generation: the targte distribution is not generated but should be learned from a real dataset.\n",
    "- The logging: use `tf.summary.*` methods to log metrics.\n",
    "- The usage of `tf.Saver` to log the summaries and to save the trained model.\n",
    "- The data visualization in tensorbard.\n",
    "- The export of the model with the correct input definition in order to be ready for serving.\n",
    "- ...\n",
    "\n",
    "In order to speed-up the development time, the tensorflow devs introduced the `tf.estimator`<sup>[1](#1)</sup> and `tf.data`<sup>[2](#1)</sup> API: an high level API that greatly simplifies the model training.\n",
    "Estimators encapsulate the following actions:\n",
    "\n",
    "- training\n",
    "- evaluation\n",
    "- prediction\n",
    "- export for serving\n",
    "\n",
    "While the `tf.data` API made the dataset generation extremely easy.\n",
    "Moreover, in tensorflow 2.0 a new (and huge!) set of performance-optimized and well-designed dataset has been added: tensorflow-dataset (`tfds`).\n",
    "\n",
    "In the next chapter, we'll see how to use the `tf.estimator` and `tfds` (that uses `tf.data`) to train a GAN.\n",
    "\n",
    "We'll start with the definition of the data and of the discriminator.\n",
    "\n",
    "## Bonus exercise: converting it to a Conditional GAN\n",
    "\n",
    "Extending this simple GAN making it conditional can be a good test bench to check your understanding of the GAN theory explained in the first section.\n",
    "\n",
    "Remeber that both $G$ and $D$ needs to be conditioned and that the train process is just the same game, played for every different condition.\n",
    "\n",
    "Copy this notebook and try to condition this GAN in order to learn 2 different normal distributions.\n",
    "\n",
    "- Condition `0`: generates $\\mathcal{N}(10, 0.1)$\n",
    "- Condition `1`: generates $\\mathcal{N}(20, 0.2)$\n",
    "\n",
    "A solution to this exersice can be found in the notebook: [BONUS - Conditional GAN from scratch](BONUS - Conditional GAN from scratch.ipynb).\n",
    "\n",
    "---\n",
    "<a id=\"1\">[1]</a>: https://www.tensorflow.org/guide/estimators\n",
    "\n",
    "<a id=\"2\">[2]</a>: https://www.tensorflow.org/guide/datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "330px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
