{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/logo.jpg\" style=\"width:85px;height:85px;float:left\" /><h1 style=\"position:relative;float:left;display:inline\">Writing an GAN from scratch</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://colab.research.google.com/github/zurutech/gans-from-theory-to-production/blob/master/2.%20GANs%20in%20Tensorflow/2.1.%20Writing%20a%20GAN%20from%20scratch.ipynb'>\n",
    "    <img align=\"left\" src='https://cdn-images-1.medium.com/max/800/1*ZpNn76K98snC9vDiIJ6Ldw.jpeg'></img>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "! pip install --upgrade tensorflow-gpu==2.0.0beta1\n",
    "# ! pip install --upgrade tensorflow==2.0.0beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Writing-an-GAN-from-scratch\" data-toc-modified-id=\"Writing-an-GAN-from-scratch-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Writing an GAN from scratch</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-does-a-GAN-learn?\" data-toc-modified-id=\"What-does-a-GAN-learn?-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>What does a GAN learn?</a></span></li><li><span><a href=\"#Input-data\" data-toc-modified-id=\"Input-data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Input data</a></span></li><li><span><a href=\"#Generator-and-discriminator-networks:-Keras-functional-API\" data-toc-modified-id=\"Generator-and-discriminator-networks:-Keras-functional-API-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Generator and discriminator networks: Keras functional API</a></span></li><li><span><a href=\"#Define-input-and-instantiate-networks\" data-toc-modified-id=\"Define-input-and-instantiate-networks-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Define input and instantiate networks</a></span></li><li><span><a href=\"#The-loss-function-and-the-training-procedure\" data-toc-modified-id=\"The-loss-function-and-the-training-procedure-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>The loss function and the training procedure</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discriminator-loss-function\" data-toc-modified-id=\"Discriminator-loss-function-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Discriminator loss function</a></span></li><li><span><a href=\"#Generator-loss-function\" data-toc-modified-id=\"Generator-loss-function-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Generator loss function</a></span></li></ul></li><li><span><a href=\"#Gradient-ascent\" data-toc-modified-id=\"Gradient-ascent-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Gradient ascent</a></span><ul class=\"toc-item\"><li><span><a href=\"#Visualize-training\" data-toc-modified-id=\"Visualize-training-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Visualize training</a></span></li><li><span><a href=\"#Advantages-and-disadvantages\" data-toc-modified-id=\"Advantages-and-disadvantages-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;</span>Advantages and disadvantages</a></span></li></ul></li></ul></li><li><span><a href=\"#Bonus-exercise:-converting-it-to-a-Conditional-GAN\" data-toc-modified-id=\"Bonus-exercise:-converting-it-to-a-Conditional-GAN-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Bonus exercise: converting it to a Conditional GAN</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing an GAN from scratch\n",
    "\n",
    "Previous to version 2.0, Tensorflow allowed us to only define computational graphs. We could hence describe both the generator $G$ and the discriminator $D$, just like two interacting subgraphs.\n",
    "\n",
    "Keras changes the way of reasoning, making the model definition more pythonic: before Keras we had to reason thinking about graphs and scopes; using Keras we have to think about objects and attributes.\n",
    "\n",
    "In Keras a variable lives inside the model, in pure old Tensorflow instead, the variable lives in the global graph.\n",
    "\n",
    "The Keras way of defining the Models is the new standard.\n",
    "\n",
    "### What does a GAN learn?\n",
    "\n",
    "The aim of the GAN we are going to implement is to learn a certain **data distribution** present in the training set.\n",
    "\n",
    "The following example will guide you trough the required steps to build a GAN from scratch using Tensorflow while giving you an **intuition** of what a GAN learns.\n",
    "\n",
    "### Input data\n",
    "\n",
    "In order to give a meaningful example, we're going to generate and visualize the distribution that has to be learn.\n",
    "\n",
    "$$ \\mathcal{N}(\\mu = 10, \\sigma = 0.1) $$\n",
    "\n",
    "Our **target** distribution is a Gaussian distribution centered in $10$ a with a standard deviation of $0.1$\n",
    "\n",
    "We're going to use Tensorflow itself to **sample datapoints** from this distribution in order to build our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required includes to create the GAN and to visualize the data\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 2.0 defaults to eager mode execution, therefore we can just use Tensorflow as we are used to use other libraries such as `numpy` to generate a dataset.\n",
    "We can thus define a function that samples from our target distribution of fixed number of data points (2000).\n",
    "\n",
    "Note: **Eager execution** means that the operations are immediately run and return concrete values instead of constructing a computational graph to run later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 60)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sample_dataset():\n",
    "    dataset_shape = (2000, 1)\n",
    "    return tf.random.normal(mean=10., shape=dataset_shape, stddev=0.1, dtype=tf.float32)\n",
    "\n",
    "counts, bin, ignored = plt.hist(sample_dataset().numpy(), 100)\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1,11])\n",
    "axes.set_ylim([0, 60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator and discriminator networks: Keras functional API\n",
    "\n",
    "The **generator** $G$ network architecture is **completely arbitrary**: practice suggests that a simple network with just 2 fully connected layers and a single linear layer at the output is enough for learning such a simple task.\n",
    "\n",
    "The **discriminator** $D$ network architecture is **completely arbitrary** too. \n",
    "\n",
    "Note: We could have apply the sigmoid but we left that task to the **BinaryCrossEntropy** Keras function. We just indicate that we input values as logits (i.e., non-normalized results).\n",
    "\n",
    "There are three ways to construct a Keras model: \n",
    "\n",
    "* Sequential \n",
    "* Functional \n",
    "* Subclassing\n",
    "\n",
    "and we use the functional API.\n",
    "In this formulation we can just use the Keras layers as functions: they accept an input and produce an output - easy.\n",
    "\n",
    "The **discriminator** $D$ network architecture is **completely arbitrary** too. \n",
    "\n",
    "Note: We could have apply the sigmoid but we left that task to the **BinaryCrossEntropy** Keras function. We just indicate that we input values as logits (i.e., non-normalized results).\n",
    "\n",
    "There are three ways to construct a Keras model: \n",
    "\n",
    "* Sequential \n",
    "* Functional \n",
    "* Subclassing\n",
    "\n",
    "and we use the functional API.\n",
    "In this formulation we can just use the Keras layers as functions: they accept an input and produce an output - easy.\n",
    "\n",
    "Indeed, for our simple model definition, a functional approach is not strictly required since a simpler interface like `Sequential` could be enough; however, we described the model using the functional approach because we found it more powerful and architecture-change ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(input_shape):\n",
    "    \"\"\"Defines the generator keras.Model.\n",
    "    Args:\n",
    "        input_shape: the desired input shape (e.g.: (latent_space_size))\n",
    "    Returns:\n",
    "        G: The generator model\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    net = tf.keras.layers.Dense(units=64, activation=tf.nn.elu, name=\"fc1\")(inputs)\n",
    "    net = tf.keras.layers.Dense(units=64, activation=tf.nn.elu, name=\"fc2\")(net)\n",
    "    net = tf.keras.layers.Dense(units=1, name=\"G\")(net)\n",
    "    G = tf.keras.Model(inputs=inputs, outputs=net)\n",
    "    return G\n",
    "\n",
    "def build_disciminator(input_shape):\n",
    "    \"\"\"Defines the discriminator keras.Model.\n",
    "    Args:\n",
    "        input_shape: the desired input shape (e.g.: (the generator output shape))\n",
    "    Returns:\n",
    "        D: the discriminator model\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    net = tf.keras.layers.Dense(units=32, activation=tf.nn.elu, name=\"fc1\")(inputs)\n",
    "    net = tf.keras.layers.Dense(units=1, name=\"D\")(net)\n",
    "    D = tf.keras.Model(inputs=inputs, outputs=net)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input and instantiate networks\n",
    "\n",
    "The two functions just defined allow us to instantiate the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the real input shape, a batch of values sampled from the real data\n",
    "input_shape = (1,)\n",
    "\n",
    "# Define the discriminator model\n",
    "D = build_disciminator(input_shape)\n",
    "\n",
    "# Arbitrary set the shape of the noise prior\n",
    "latent_space_shape = (100,)\n",
    "\n",
    "# Define the input noise shape and define the generator\n",
    "G = build_generator(latent_space_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss function and the training procedure\n",
    "\n",
    "As the theory describes, the value function of GAN is represented by min-max zero-sum game detailed as:\n",
    "\n",
    "$$ \\min_{G} \\max_{D} V(D, G)=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}}(\\boldsymbol{z})[\\log (1-D(G(\\boldsymbol{z})))] $$\n",
    "\n",
    "The training procedure is the **alternating** execution of training steps and the loss value is calculated using cross-entropy `tf.keras.losses.BinaryCrossentropy(from_logits=True)`. \n",
    "\n",
    "In practice, the `tf.keras.losses.BinaryCrossentropy(from_logits=True)` computes the binary cross entropy between two distributions:\n",
    "\n",
    "1. the learned distribution, that assigns a probability to a certain class, that's why the function applies the **sigmoid**  $\\sigma$ function to the output neuron, in order to consider the output a probability (range [0-1])\n",
    "2. the conditional empirical distribution over class labels (a probability distribution where the probability of the current observed positive sample is 1 and is 0 for any other class)\n",
    "\n",
    "$$ \\mathcal{L}_{BCE} = - y \\log(\\hat{y}) - (1 - y)\\log(1 - \\hat{y}) $$\n",
    "\n",
    "In particular, we're going to implement the **non saturating** version of the value function.\n",
    "\n",
    "#### Discriminator loss function\n",
    "\n",
    "Given the labels for the positive and negative class to be 1 and 0 respectively, the loss becomes the sum of 2 BCE:\n",
    "\n",
    "$$ \\frac{1}{m} \\sum_{i=1}^{m}- \\log \\sigma(D(x^{(i)})) + \\frac{1}{m} \\sum_{i=1}^{m} - \\log(1 - \\sigma(D(G(z^{(i)})) $$\n",
    "\n",
    "The loss function is sum of the binary cross entropy on the class of the real samples (label 1) and the binary cross entropy on the class of the generated samples (label 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_loss(real_output, generated_output):\n",
    "    \"\"\"The discriminator loss function.\"\"\"\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    return bce(tf.ones_like(real_output), real_output) + bce(tf.zeros_like(generated_output), generated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator loss function\n",
    "$$ -\\frac{1}{m} \\sum_{i=1}^{m} \\sigma(\\log(D(G(z)))) $$\n",
    "\n",
    "The loss function is just the binary cross entropy between the log probability of the generated images and the distribution of the real images (label 1)\n",
    "\n",
    "As previously stated, we are using here the **non saturating** version instead of the theoretical version $$ -\\frac{1}{m} \\sum_{i=1}^{m} \\sigma(\\log(1 - D(G(z)))) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_loss(generated_output):\n",
    "    \"\"\"The Generator loss function.\"\"\"\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    return bce(tf.ones_like(generated_output), generated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non saturating** version of the minimax game lets the generator maximize the log-probability of the discriminator \n",
    "being mistaken instead of minimizing the log-probability of being correct. Hence, we approach the problem with a Gradient Ascent approach.\n",
    "\n",
    "### Gradient ascent\n",
    "\n",
    "To train 2 different networks one next to the other, we can exploit the fact that we can specify to an optimizer the \n",
    "list of variables to be trained and it will take care of updating only that variables. Using Keras, this is for free, \n",
    "since every single Model brings its own set of `trainable_variables`.\n",
    "\n",
    "The training process, thanks to the eager mode, is **exactly** the one described in the GAN paper.\n",
    "\n",
    "The `@tf.function` annotation allows to convert the most computational-intensive part of the training into its graph \n",
    "representation and accelerate, thus, the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not os.path.exists(\"./gif/\"):\n",
    "     os.makedirs(\"./gif/\")\n",
    "\n",
    "# Let's play the min-max game\n",
    "def train():\n",
    "    # Define the optimizers and the train operations\n",
    "    optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step():\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            real_data = sample_dataset()\n",
    "            noise_vector = tf.random.normal(\n",
    "                mean=0, stddev=1, shape=(real_data.shape[0], latent_space_shape[0])\n",
    "            )\n",
    "            # Sample from the Generator\n",
    "            fake_data = G(noise_vector)\n",
    "            # Compute the D loss\n",
    "            d_fake_data = D(fake_data)\n",
    "            d_real_data = D(real_data)\n",
    "            d_loss_value = d_loss(generated_output=d_fake_data, real_output=d_real_data)\n",
    "            # Compute the G loss\n",
    "            g_loss_value = g_loss(generated_output=d_fake_data)\n",
    "        # Now that we computed the losses we can compute the gradient (using the tape)\n",
    "        # and optimize the networks\n",
    "        d_gradients = tape.gradient(d_loss_value, D.trainable_variables)\n",
    "        g_gradients = tape.gradient(g_loss_value, G.trainable_variables)\n",
    "        del tape\n",
    "        \n",
    "        # Apply gradients to variables\n",
    "        optimizer.apply_gradients(zip(d_gradients, D.trainable_variables))\n",
    "        optimizer.apply_gradients(zip(g_gradients, G.trainable_variables))\n",
    "        return real_data, fake_data, g_loss_value, d_loss_value\n",
    "    \n",
    "    # 40000 training steps with logging every 200 steps\n",
    "    fig, ax = plt.subplots()\n",
    "    for step in range(40000):\n",
    "        real_data, fake_data,g_loss_value, d_loss_value = train_step()\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"G loss: \", \n",
    "                g_loss_value.numpy(), \n",
    "                \" D loss: \", \n",
    "                d_loss_value.numpy(), \n",
    "                \" step: \", \n",
    "                step,\n",
    "            )\n",
    "\n",
    "            # Sample 5000 values from the Generator and draw the histogram\n",
    "            ax.hist(fake_data.numpy(), 100)\n",
    "            ax.hist(real_data.numpy(), 100)\n",
    "            # these are matplotlib.patch.Patch properties\n",
    "            props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "            # place a text box in upper left in axes coords\n",
    "            textstr = f\"step={step}\"\n",
    "            ax.text(\n",
    "                0.05, \n",
    "                0.95, \n",
    "                textstr, \n",
    "                transform=ax.transAxes, \n",
    "                fontsize=14,\n",
    "                verticalalignment='top', \n",
    "                bbox=props,\n",
    "            )\n",
    "\n",
    "            axes = plt.gca()\n",
    "            axes.set_xlim([-1,11])\n",
    "            axes.set_ylim([0, 60])\n",
    "            display.display(pl.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            plt.savefig(\"./gif/{}.png\".format(step))\n",
    "            plt.gca().clear()\n",
    "            \n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize training\n",
    "\n",
    "Every 200 steps of train we generated and saved the histogram of 2000 values sampled from the target distribution and 2000 values sampled from the generator output.\n",
    "\n",
    "\n",
    "In order to generate a nice animation of the learning process we can just use the `imagemagick` tool to merge all the images into a pretty cool gif of the learning process.\n",
    "This gif shows you how the learning process learned to shift the initial random distribution to the correct value of $10$ and to adjust the variance in order to be close to the one of **target distribution**.\n",
    "\n",
    "\n",
    "```python\n",
    "# ! convert -delay 20 -loop 0 $(ls gif/*.png | sort -V) gif/learning_gaussian.gif\n",
    "```\n",
    "\n",
    "\n",
    "![learn](gif/learning_gaussian.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages and disadvantages\n",
    "\n",
    "Writing a GAN from scratch using the new Tensorflow 2.0 API is extremely helpful for learning how to correctly define our own training loop, how the write a GAN, and understand the theory required to make the network learn.\n",
    "\n",
    "However, this example could become way more complex if we add:\n",
    "\n",
    "- The dataset generation: the target distribution is not generated but should be learned from a real dataset.\n",
    "- The logging: use `tf.summary.*` methods to log metrics.\n",
    "- The usage of `tf.Saver` to log the summaries and to save the trained model.\n",
    "- The data visualization in Tensorbard.\n",
    "- The export of the model with the correct input definition in order to be ready for serving.\n",
    "- ...\n",
    "\n",
    "In order to speed-up the development time, the Tensorflow devs introduced `tensorflow-datasets`<sup>[1](#1)</sup> and we at Zuru Tech introduced AshPy.\n",
    "\n",
    "The former, `tfds` is a collection of datasets ready to use as `tf.data.Dataset` objects <sup>[2](#1)</sup>, while the latter is a pure TensorFlow 2.0 library for distributed training, evaluation, model selection and fast prototyping.\n",
    "\n",
    "AshPy encapsulate the following actions:\n",
    "\n",
    "- training\n",
    "- evaluation\n",
    "- prediction\n",
    "- model selection\n",
    "- export for serving (feature inherited from the Keras models)\n",
    "\n",
    "\n",
    "In the next chapter, we'll see how to use the `AshPy` and `tfds` to train a GAN.\n",
    "\n",
    "We'll start with the definition of the data and of the discriminator.\n",
    "\n",
    "## Bonus exercise: converting it to a Conditional GAN\n",
    "\n",
    "Extending this simple GAN making it conditional can be a good test bench to check your understanding of the GAN theory explained in the first section.\n",
    "\n",
    "Remember that both $G$ and $D$ needs to be conditioned and that the train process is just the same game, played for every different condition.\n",
    "\n",
    "Copy this notebook and try to condition this GAN in order to learn 2 different normal distributions.\n",
    "\n",
    "- Condition `0`: generates $\\mathcal{N}(10, 0.1)$\n",
    "- Condition `1`: generates $\\mathcal{N}(20, 0.2)$\n",
    "\n",
    "A solution to this exercise can be found in the notebook: [BONUS - Conditional GAN from scratch](BONUS%20-%20Conditional%20GAN%20from%20scratch.ipynb).\n",
    "\n",
    "---\n",
    "<a id=\"1\">[1]</a>: https://www.tensorflow.org/datasets\n",
    "\n",
    "<a id=\"2\">[2]</a>: https://www.tensorflow.org/guide/datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "330px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
